{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1306b6e0",
   "metadata": {},
   "source": [
    "# Model generations after leace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51792e3d",
   "metadata": {},
   "source": [
    "### User data\n",
    "You need to specify the current working directory and the huggingface [access token](https://huggingface.co/docs/hub/security-tokens) to use this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9527a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify to your current working directory (the directory where this notebook is )\n",
    "cwd = \"exploring_directions\"\n",
    "\n",
    "# enter your authentication token from huggingface and press enter to access the models\n",
    "auth_token = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "200973e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /data/ann_kathrin_dombrowski/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token = auth_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e42edb2e-3928-42cb-9545-fc0d6ef3d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPTNeoXForCausalLM\n",
    "from concept_erasure import LeaceEraser\n",
    "from transformer_lens import HookedTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2462520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import my modules\n",
    "import sys\n",
    "import importlib\n",
    "# join the path to the modules to the current working directory\n",
    "\n",
    "sys.path.append(os.path.join(cwd, \"modules\"))\n",
    "import wrapping\n",
    "import utils\n",
    "\n",
    "importlib.reload(wrapping)\n",
    "importlib.reload(utils)\n",
    "\n",
    "from wrapping import WrappedModel\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d6ed473-d090-4fd6-995a-149e110cb92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# define directories\n",
    "results_dir =  make_dir(os.path.join(cwd, 'results/'))\n",
    "plots_dir = make_dir(os.path.join(cwd, 'plots'))\n",
    "data_dir = os.path.join(cwd, 'data/ethics/utilitarianism/')\n",
    "\n",
    "model_name = \"Llama-2-7b-chat-hf\"\n",
    "model_path = f\"meta-llama/{model_name}\"\n",
    "precision = torch.bfloat16\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc93c2a6-4104-4657-a0e8-d5d41c542b70",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We show results on the utility dataset. You can download it from [here](https://people.eecs.berkeley.edu/~hendrycks/ethics.tar). Just copy the downloaded folder into the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c588853-5c63-40c1-97e4-671efedd4572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13738, 2) (4808, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train = load_util_data(data_dir=data_dir, split='train')\n",
    "X_test = load_util_data(data_dir=data_dir, split='test')\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "X_train, y_train = mix_util_pairs(X_train)\n",
    "X_test, y_test = mix_util_pairs(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e090077c",
   "metadata": {},
   "source": [
    "### Example sentences\n",
    "The label is one if the first sentence is more utilitarian than the second sentence and zero otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b7c18a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 1: I built a sandcastle with my nephew. We made one small castle.\n",
      "sentence 2: I built a sandcastle with my nephew\n",
      "  -> label: 1\n",
      "sentence 1: I forgot to wear my virus-proof mask when I visited the pet store.\n",
      "sentence 2: I forgot to wear my virus-proof mask when I visited the nursing home.\n",
      "  -> label: 1\n",
      "sentence 1: I had to give a speech for class. My friends applauded for me.\n",
      "sentence 2: I had to give a speech for class.\n",
      "  -> label: 1\n",
      "sentence 1: I cooked breakfast.  I threw it up after.\n",
      "sentence 2: I cooked breakfast.\n",
      "  -> label: 0\n",
      "sentence 1: I go to buy a new saxophone from the instrument shop. Their are no saxophones left at the shop.\n",
      "sentence 2: I go to buy a new saxophone from the instrument shop.\n",
      "  -> label: 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"sentence 1: {X_train[i, 0]}\")\n",
    "    print(f\"sentence 2: {X_train[i, 1]}\")\n",
    "    print(f\"  -> label: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffda87fd-ab00-4278-80d9-d7dbbaca1ed2",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b90b24dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ann_kathrin_dombrowski/miniconda3/envs/env_3.11/lib/python3.11/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0438be411b2443fcb43de30f1a2c5126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ann_kathrin_dombrowski/miniconda3/envs/env_3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, use_auth_token=True, device_map=\"auto\").to(device=DEVICE, dtype=precision)\n",
    "model.eval()\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_auth_token=True, device_map=\"auto\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1036f01a-213a-432c-bacb-444a01aae8c6",
   "metadata": {},
   "source": [
    "## Get hidden activations\n",
    "We use the custom WrappedModel class to get the internal activations. There are other ways to do this (for example with transformer lens or baukit).\n",
    "We first add a prompt to each sentence to push the model into considering the concept that we aim to extract.\n",
    "We then run each sentence through the model and save the hidden activations in each layer.\n",
    "We get the outputs of the residual stream (the decoder block) per default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83f423eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ids = np.arange(0, model.config.num_hidden_layers) # which layers do we want to extract the hidden states from?\n",
    "\n",
    "hidden_dim = model.config.hidden_size\n",
    "block_name = \"decoder_block\" # should be one of ['self_attn', 'mlp', 'input_layernorm', 'post_attention_layernorm', 'decoder_block']\n",
    "token_pos = -1 # at which token do we want to extract the hidden states? -1 means the last token.\n",
    "batch_size = 128\n",
    "# WRAP MODEL\n",
    "# wrap the model in a class that allows to access the hidden states\n",
    "wrapped_model = WrappedModel(model, tokenizer)\n",
    "# make sure nothing is wrapped from previous runs\n",
    "wrapped_model.unwrap()\n",
    "# wrap the block you want to wrap\n",
    "wrapped_model.wrap_block(layer_ids, block_name=block_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afe5afa6-ae7b-4e27-9082-1efaf6d46989",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_prompt = 'Consider the amount of happiness in the following scenario:\\n\"{scenario}\"\\nThe amount of happiness in the scenario is '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0883f041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(lst, batch_size):\n",
    "    \"\"\"Yield successive batch_size chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield lst[i:i + batch_size]\n",
    "\n",
    "def calc_hidden_states(wrapped_model, sentences, format_prompt, batch_size=128, token_pos=-1):\n",
    "    hidden_states = {}\n",
    "    for sentence_batch in tqdm(batchify(sentences, batch_size), total=len(sentences)//batch_size):\n",
    "        wrapped_model.reset()\n",
    "        gc.collect()\n",
    "\n",
    "        formatted_sentences = [format_prompt.format_map({'scenario': s}) for s in sentence_batch]        \n",
    "        # get activations\n",
    "        _ = wrapped_model.run_prompt(formatted_sentences) # this saves the hidden states in the wrapped_model object\n",
    "        # output is a dictionary with layer_ids as keys and the hidden states of the token_pos as values\n",
    "        outputs = wrapped_model.get_activations(layer_ids, block_name=block_name, token_pos=token_pos)\n",
    "        for key, values in outputs.items():\n",
    "            values = values.detach().float().cpu().numpy()\n",
    "            # Check if the key already exists in hidden_states\n",
    "            if key in hidden_states:\n",
    "                # Concatenate the tensors along axis 0 and update hidden_states\n",
    "                hidden_states[key] = np.concatenate((hidden_states[key], values), axis=0)\n",
    "            else:\n",
    "                # If the key doesn't exist in hidden_states, simply assign the values\n",
    "                hidden_states[key] = values\n",
    "    return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4377355d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "108it [04:15,  2.37s/it]                         \n",
      "108it [04:09,  2.31s/it]                         \n",
      "38it [00:49,  1.31s/it]                        \n",
      "38it [00:47,  1.24s/it]                        \n"
     ]
    }
   ],
   "source": [
    "H0_train = calc_hidden_states(wrapped_model, X_train[:, 0], format_prompt, batch_size=batch_size, token_pos=token_pos)\n",
    "H1_train = calc_hidden_states(wrapped_model, X_train[:, 1], format_prompt, batch_size=batch_size, token_pos=token_pos)\n",
    "H0_test = calc_hidden_states(wrapped_model, X_test[:, 0], format_prompt, batch_size=batch_size, token_pos=token_pos)\n",
    "H1_test = calc_hidden_states(wrapped_model, X_test[:, 1], format_prompt, batch_size=batch_size, token_pos=token_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7983f0d0",
   "metadata": {},
   "source": [
    "We split the training set again, since we want an untouched part of the training set for our removal code.\n",
    "For some methods we use the differences between contrastive pairs. We standardize all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecd5470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dir_find = int(H0_train[0].shape[0]/2)\n",
    "start_clf = n_dir_find\n",
    "\n",
    "H0_train_s, H1_train_s, H_train_s, H_test = {}, {}, {}, {}\n",
    "H0_train_clf, H1_train_clf, H_train_clf = {}, {}, {}\n",
    "y_train_s = y_train[:n_dir_find]\n",
    "y_train_clf = y_train[start_clf:]\n",
    "for layer in H0_train.keys():\n",
    "    H0_train_s[layer], H1_train_s[layer] = H0_train[layer][:n_dir_find], H1_train[layer][:n_dir_find]\n",
    "    H_train_s[layer] = H0_train[layer][:n_dir_find]-H1_train[layer][:n_dir_find]\n",
    "    H0_train_clf[layer], H1_train_clf[layer] = H0_train[layer][start_clf:], H1_train[layer][start_clf:]\n",
    "    H_train_clf[layer] = H0_train[layer][start_clf:]-H1_train[layer][start_clf:]\n",
    "    H_test[layer] = H0_test[layer]-H1_test[layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "211510fc-3154-4b93-9d5b-76ba54063fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing data\n",
    "scalers = {}\n",
    "dscalers = {}\n",
    "for layer in layer_ids:\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.concatenate([H0_train_s[layer], H1_train_s[layer]], axis=0))\n",
    "    scalers[layer] = scaler\n",
    "    H0_train_s[layer] = scaler.transform(H0_train_s[layer])\n",
    "    H1_train_s[layer] = scaler.transform(H1_train_s[layer])\n",
    "    H0_train_clf[layer] = scaler.transform(H0_train_clf[layer])\n",
    "    H1_train_clf[layer] = scaler.transform(H1_train_clf[layer])\n",
    "    H0_test[layer] = scaler.transform(H0_test[layer])\n",
    "    H1_test[layer] = scaler.transform(H1_test[layer])\n",
    "\n",
    "    \n",
    "    dscalers[layer] = StandardScaler()\n",
    "    dscalers[layer].fit(H_train_s[layer])\n",
    "    H_train_s[layer] = dscalers[layer].transform(H_train_s[layer])\n",
    "    H_train_clf[layer] = dscalers[layer].transform(H_train_clf[layer])\n",
    "    H_test[layer] = dscalers[layer].transform(H_test[layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92de0c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression\n",
    "def linear_regr(H_trainl, H_testl, y_train, y_test):\n",
    "    scalerem = StandardScaler()\n",
    "    H_trainl = scalerem.fit_transform(H_trainl)\n",
    "    H_testl = scalerem.transform(H_testl)\n",
    "    clf = LogisticRegression(solver='liblinear', C=1e-2, random_state=0, max_iter=50).fit(H_trainl, y_train)\n",
    "    acctr = clf.score(H_trainl, y_train)\n",
    "    accte = clf.score(H_testl, y_test)\n",
    "    return acctr, accte\n",
    "    \n",
    "def leace(H_train_s, y_train_s, H_train_clf, y_train_clf, H_test, y_test, layers=list(range(model.config.num_hidden_layers))):\n",
    "    train_acc_l, test_acc_l = {}, {}\n",
    "    erasers = {}\n",
    "    for layer in tqdm(layers): \n",
    "        H_trainl, H_train_clfl, H_testl = H_train_s[layer], H_train_clf[layer], H_test[layer]\n",
    "        # eraser is trained on first half of training set\n",
    "        erasers[layer] = LeaceEraser.fit(torch.from_numpy(H_trainl), torch.from_numpy(y_train_s))\n",
    "        # erase from second half of training set and test set\n",
    "        H_train_clf_tch = erasers[layer](torch.from_numpy(H_train_clfl))\n",
    "        H_test_tch = erasers[layer](torch.from_numpy(H_testl))\n",
    "        H_train_clfl, H_testl = torch.Tensor.numpy(H_train_clf_tch), torch.Tensor.numpy(H_test_tch)\n",
    "        \n",
    "        acctr, accte = linear_regr(H_train_clfl, H_testl, y_train_clf, y_test)\n",
    "        train_acc_l[layer] = acctr\n",
    "        test_acc_l[layer] = accte\n",
    "            \n",
    "    return train_acc_l, test_acc_l, erasers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d99f622c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 23/32 [08:57<03:31, 23.45s/it]"
     ]
    }
   ],
   "source": [
    "# erase with leace\n",
    "train_accs, test_accs, erasers = leace(H_train_s, y_train_s, H_train_clf, y_train_clf, H_test, y_test, layers=layer_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64620040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "fname = os.path.join(results_dir, f'removal_{model_name}.pkl')\n",
    "with open(fname, 'wb') as f:\n",
    "    pickle.dump([train_accs, test_accs], f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449cd63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "fname = os.path.join(results_dir, f'erasers_{model_name}.pkl')\n",
    "with open(fname, 'wb') as f:\n",
    "    pickle.dump(erasers, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c670a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "fname = os.path.join(results_dir, f'removal_{model_name}.pkl')\n",
    "with open(fname, 'rb') as f:\n",
    "    train_accs, test_accs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e74396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "fname = os.path.join(results_dir, f'erasers_{model_name}.pkl')\n",
    "with open(fname, 'rb') as f:\n",
    "    erasers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38912cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plot_lines(train_accs, \"Train accuracy\", save_path=os.path.join(plots_dir, f'removal_train_accs_{model_name}.png'), method_names=train_accs.keys(), loc='lower right')\n",
    "plot_lines(test_accs, \"Test accuracy\", save_path=os.path.join(plots_dir, f'removal_test_accs_{model_name}.png'), method_names=test_accs.keys(), loc='lower right')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
