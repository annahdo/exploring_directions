{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1306b6e0",
   "metadata": {},
   "source": [
    "# Model generations after leace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51792e3d",
   "metadata": {},
   "source": [
    "### User data\n",
    "You need to specify the current working directory and the huggingface [access token](https://huggingface.co/docs/hub/security-tokens) to use this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9527a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify to your current working directory (the directory where this notebook is )\n",
    "cwd = \"exploring_directions\"\n",
    "\n",
    "# enter your authentication token from huggingface and press enter to access the models\n",
    "auth_token = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42edb2e-3928-42cb-9545-fc0d6ef3d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPTNeoXForCausalLM\n",
    "from concept_erasure import LeaceEraser\n",
    "from transformer_lens import HookedTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2462520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import my modules\n",
    "import sys\n",
    "import importlib\n",
    "# join the path to the modules to the current working directory\n",
    "\n",
    "sys.path.append(os.path.join(cwd, \"modules\"))\n",
    "import wrapping\n",
    "import utils\n",
    "\n",
    "importlib.reload(wrapping)\n",
    "importlib.reload(utils)\n",
    "\n",
    "from wrapping import WrappedModel\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6ed473-d090-4fd6-995a-149e110cb92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directories\n",
    "results_dir =  make_dir(os.path.join(cwd, 'results/'))\n",
    "plots_dir = make_dir(os.path.join(cwd, 'plots'))\n",
    "data_dir = os.path.join(cwd, 'data/ethics/utilitarianism/')\n",
    "\n",
    "model_name = \"Llama-2-7b-chat-hf\"\n",
    "model_path = f\"meta-llama/{model_name}\"\n",
    "precision = torch.bfloat16\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc93c2a6-4104-4657-a0e8-d5d41c542b70",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We show results on the utility dataset. You can download it from [here](https://people.eecs.berkeley.edu/~hendrycks/ethics.tar). Just copy the downloaded folder into the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c588853-5c63-40c1-97e4-671efedd4572",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load_util_data(data_dir=data_dir, split='train')\n",
    "X_test = load_util_data(data_dir=data_dir, split='test')\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "X_train, y_train = mix_util_pairs(X_train)\n",
    "X_test, y_test = mix_util_pairs(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e090077c",
   "metadata": {},
   "source": [
    "### Example sentences\n",
    "The label is one if the first sentence is more utilitarian than the second sentence and zero otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7c18a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(f\"sentence 1: {X_train[i, 0]}\")\n",
    "    print(f\"sentence 2: {X_train[i, 1]}\")\n",
    "    print(f\"  -> label: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffda87fd-ab00-4278-80d9-d7dbbaca1ed2",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90b24dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, token=auth_token, device_map=\"auto\").to(device=DEVICE, dtype=precision)\n",
    "model.eval()\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, token=auth_token, device_map=\"auto\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1036f01a-213a-432c-bacb-444a01aae8c6",
   "metadata": {},
   "source": [
    "## Get hidden activations\n",
    "We use the custom WrappedModel class to get the internal activations. There are other ways to do this (for example with transformer lens or baukit).\n",
    "We first add a prompt to each sentence to push the model into considering the concept that we aim to extract.\n",
    "We then run each sentence through the model and save the hidden activations in each layer.\n",
    "We get the outputs of the residual stream (the decoder block) per default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f423eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ids = np.arange(0, model.config.num_hidden_layers) # which layers do we want to extract the hidden states from?\n",
    "\n",
    "hidden_dim = model.config.hidden_size\n",
    "block_name = \"decoder_block\" # should be one of ['self_attn', 'mlp', 'input_layernorm', 'post_attention_layernorm', 'decoder_block']\n",
    "token_pos = -1 # at which token do we want to extract the hidden states? -1 means the last token.\n",
    "batch_size = 128\n",
    "# WRAP MODEL\n",
    "# wrap the model in a class that allows to access the hidden states\n",
    "wrapped_model = WrappedModel(model, tokenizer)\n",
    "# make sure nothing is wrapped from previous runs\n",
    "wrapped_model.unwrap()\n",
    "# wrap the block you want to wrap\n",
    "wrapped_model.wrap_block(layer_ids, block_name=block_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe5afa6-ae7b-4e27-9082-1efaf6d46989",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_prompt = 'Consider the amount of happiness in the following scenario:\\n\"{scenario}\"\\nThe amount of happiness in the scenario is '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0883f041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(lst, batch_size):\n",
    "    \"\"\"Yield successive batch_size chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield lst[i:i + batch_size]\n",
    "\n",
    "def calc_hidden_states(wrapped_model, sentences, format_prompt, batch_size=128, token_pos=-1):\n",
    "    hidden_states = {}\n",
    "    for sentence_batch in tqdm(batchify(sentences, batch_size), total=len(sentences)//batch_size):\n",
    "        wrapped_model.reset()\n",
    "        gc.collect()\n",
    "\n",
    "        formatted_sentences = [format_prompt.format_map({'scenario': s}) for s in sentence_batch]        \n",
    "        # get activations\n",
    "        _ = wrapped_model.run_prompt(formatted_sentences) # this saves the hidden states in the wrapped_model object\n",
    "        # output is a dictionary with layer_ids as keys and the hidden states of the token_pos as values\n",
    "        outputs = wrapped_model.get_activations(layer_ids, block_name=block_name, token_pos=token_pos)\n",
    "        for key, values in outputs.items():\n",
    "            values = values.detach().float().cpu().numpy()\n",
    "            # Check if the key already exists in hidden_states\n",
    "            if key in hidden_states:\n",
    "                # Concatenate the tensors along axis 0 and update hidden_states\n",
    "                hidden_states[key] = np.concatenate((hidden_states[key], values), axis=0)\n",
    "            else:\n",
    "                # If the key doesn't exist in hidden_states, simply assign the values\n",
    "                hidden_states[key] = values\n",
    "    return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4377355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "H0_train = calc_hidden_states(wrapped_model, X_train[:, 0], format_prompt, batch_size=batch_size, token_pos=token_pos)\n",
    "H1_train = calc_hidden_states(wrapped_model, X_train[:, 1], format_prompt, batch_size=batch_size, token_pos=token_pos)\n",
    "H0_test = calc_hidden_states(wrapped_model, X_test[:, 0], format_prompt, batch_size=batch_size, token_pos=token_pos)\n",
    "H1_test = calc_hidden_states(wrapped_model, X_test[:, 1], format_prompt, batch_size=batch_size, token_pos=token_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7983f0d0",
   "metadata": {},
   "source": [
    "We split the training set again, since we want an untouched part of the training set for our removal code.\n",
    "For some methods we use the differences between contrastive pairs. We standardize all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd5470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dir_find = int(H0_train[0].shape[0]/2)\n",
    "start_clf = n_dir_find\n",
    "\n",
    "H0_train_s, H1_train_s, H_train_s, H_test = {}, {}, {}, {}\n",
    "H0_train_clf, H1_train_clf, H_train_clf = {}, {}, {}\n",
    "y_train_s = y_train[:n_dir_find]\n",
    "y_train_clf = y_train[start_clf:]\n",
    "for layer in H0_train.keys():\n",
    "    H0_train_s[layer], H1_train_s[layer] = H0_train[layer][:n_dir_find], H1_train[layer][:n_dir_find]\n",
    "    H_train_s[layer] = H0_train[layer][:n_dir_find]-H1_train[layer][:n_dir_find]\n",
    "    H0_train_clf[layer], H1_train_clf[layer] = H0_train[layer][start_clf:], H1_train[layer][start_clf:]\n",
    "    H_train_clf[layer] = H0_train[layer][start_clf:]-H1_train[layer][start_clf:]\n",
    "    H_test[layer] = H0_test[layer]-H1_test[layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211510fc-3154-4b93-9d5b-76ba54063fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing data\n",
    "scalers = {}\n",
    "dscalers = {}\n",
    "for layer in layer_ids:\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.concatenate([H0_train_s[layer], H1_train_s[layer]], axis=0))\n",
    "    scalers[layer] = scaler\n",
    "    H0_train_s[layer] = scaler.transform(H0_train_s[layer])\n",
    "    H1_train_s[layer] = scaler.transform(H1_train_s[layer])\n",
    "    H0_train_clf[layer] = scaler.transform(H0_train_clf[layer])\n",
    "    H1_train_clf[layer] = scaler.transform(H1_train_clf[layer])\n",
    "    H0_test[layer] = scaler.transform(H0_test[layer])\n",
    "    H1_test[layer] = scaler.transform(H1_test[layer])\n",
    "\n",
    "    \n",
    "    dscalers[layer] = StandardScaler()\n",
    "    dscalers[layer].fit(H_train_s[layer])\n",
    "    H_train_s[layer] = dscalers[layer].transform(H_train_s[layer])\n",
    "    H_train_clf[layer] = dscalers[layer].transform(H_train_clf[layer])\n",
    "    H_test[layer] = dscalers[layer].transform(H_test[layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92de0c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression\n",
    "def linear_regr(H_trainl, H_testl, y_train, y_test):\n",
    "    scalerem = StandardScaler()\n",
    "    H_trainl = scalerem.fit_transform(H_trainl)\n",
    "    H_testl = scalerem.transform(H_testl)\n",
    "    clf = LogisticRegression(solver='liblinear', C=1e-2, random_state=0, max_iter=50).fit(H_trainl, y_train)\n",
    "    acctr = clf.score(H_trainl, y_train)\n",
    "    accte = clf.score(H_testl, y_test)\n",
    "    return acctr, accte\n",
    "    \n",
    "def leace(H_train_s, y_train_s, H_train_clf, y_train_clf, H_test, y_test, layers=list(range(model.config.num_hidden_layers))):\n",
    "    train_acc_l, test_acc_l = {}, {}\n",
    "    erasers = {}\n",
    "    for layer in tqdm(layers): \n",
    "        H_trainl, H_train_clfl, H_testl = H_train_s[layer], H_train_clf[layer], H_test[layer]\n",
    "        # eraser is trained on first half of training set\n",
    "        erasers[layer] = LeaceEraser.fit(torch.from_numpy(H_trainl), torch.from_numpy(y_train_s))\n",
    "        # erase from second half of training set and test set\n",
    "        H_train_clf_tch = erasers[layer](torch.from_numpy(H_train_clfl))\n",
    "        H_test_tch = erasers[layer](torch.from_numpy(H_testl))\n",
    "        H_train_clfl, H_testl = torch.Tensor.numpy(H_train_clf_tch), torch.Tensor.numpy(H_test_tch)\n",
    "        \n",
    "        acctr, accte = linear_regr(H_train_clfl, H_testl, y_train_clf, y_test)\n",
    "        train_acc_l[layer] = acctr\n",
    "        test_acc_l[layer] = accte\n",
    "            \n",
    "    return train_acc_l, test_acc_l, erasers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99f622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# erase with leace\n",
    "train_accs, test_accs, erasers = leace(H_train_s, y_train_s, H_train_clf, y_train_clf, H_test, y_test, layers=layer_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64620040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "fname = os.path.join(results_dir, f'removal_{model_name}.pkl')\n",
    "with open(fname, 'wb') as f:\n",
    "    pickle.dump([train_accs, test_accs], f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449cd63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "fname = os.path.join(results_dir, f'erasers_{model_name}.pkl')\n",
    "with open(fname, 'wb') as f:\n",
    "    pickle.dump(erasers, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c670a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "fname = os.path.join(results_dir, f'removal_{model_name}.pkl')\n",
    "with open(fname, 'rb') as f:\n",
    "    train_accs, test_accs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e74396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "fname = os.path.join(results_dir, f'erasers_{model_name}.pkl')\n",
    "with open(fname, 'rb') as f:\n",
    "    erasers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31765032",
   "metadata": {},
   "source": [
    "# Steering without and with leace removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c057d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_sentences = 100\n",
    "generations_dir = make_dir(os.path.join(results_dir, 'generations_leace/'))\n",
    "\n",
    "data_dir = make_dir(os.path.join(cwd, 'data/'))\n",
    "\n",
    "data_file = os.path.join(data_dir, 'test_sentences.txt')\n",
    "steering_layer_ids = [10,15,20,25,30]\n",
    "random_seed = 42\n",
    "max_new_tokens = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2720ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data\n",
    "X_test = load_util_data(data_dir=os.path.join(cwd, 'data/ethics/utilitarianism/'), split='test')\n",
    "X_test, y_test = mix_util_pairs(X_test)\n",
    "\n",
    "test_data_idxs, test_sentences = find_two_sentences(X_test[:, 0], split_str1=\".\", split_str2=\",\", larger_than1=2, larger_than2=1)\n",
    "\n",
    "with open(data_file, \"w\") as f:\n",
    "    for s in test_sentences:\n",
    "        f.write(s + \" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8235d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open(data_file, 'r') as f:\n",
    "    test_sentences = [line.strip() for line in f]\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "idxs = np.random.choice(len(test_sentences), num_test_sentences, replace=False)\n",
    "test_sentences = [test_sentences[idx] for idx in idxs]\n",
    "for i in range(10):\n",
    "    print(test_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54904f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neutral generations\n",
    "\n",
    "generations = []\n",
    "wrapped_model.unwrap()\n",
    "for sentence_batch in batchify(test_sentences, batch_size):\n",
    "    generated = wrapped_model.generate(sentence_batch, max_new_tokens=max_new_tokens, use_cache=True)\n",
    "    generations.extend(generated)\n",
    "\n",
    "# save generations as text files\n",
    "with open(os.path.join(generations_dir, f\"generations_neutral.txt\"), \"w\") as f:\n",
    "    for item in generations:\n",
    "        # remove newline characters\n",
    "        item = item.replace(\"\\n\", \" \")\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38912cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_layer_ids = [10,15,20,25,30]\n",
    "\n",
    "for layer_id in tqdm(steering_layer_ids):\n",
    "\n",
    "    wrapped_model.unwrap()\n",
    "    wrapped_model.wrap_block(layer_id, block_name=\"decoder_block\")\n",
    "    wrapped_model.model.model.layers[layer_id].set_leace_eraser(erasers[layer_id])\n",
    "\n",
    "    generations = []\n",
    "\n",
    "    for sentence_batch in batchify(test_sentences, batch_size):\n",
    "        generated = wrapped_model.generate(sentence_batch, max_new_tokens=max_new_tokens, use_cache=True)\n",
    "        generations.extend(generated)\n",
    "\n",
    "    # save generations as text files\n",
    "    with open(os.path.join(generations_dir, f\"generations_LEACE_layer_{layer_id}.txt\"), \"w\") as f:\n",
    "        for item in generations:\n",
    "            # remove newline characters\n",
    "            item = item.replace(\"\\n\", \" \")\n",
    "            f.write(\"%s\\n\" % item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec6c397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
