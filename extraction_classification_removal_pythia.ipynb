{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1306b6e0",
   "metadata": {},
   "source": [
    "# Extracting directions, Classification and Removal\n",
    "We search for directions corresponding to a concept in hidden layer activations.\n",
    "We use several different methods:\n",
    "* One Prompt\n",
    "* Logistic Regression\n",
    "* Principal Component Analysis (PCA)\n",
    "* Class Means\n",
    "* K-Means\n",
    "* Random Direction as a baseline\n",
    "\n",
    "We check how well the directions correlate with the concept we care about by using them to separate the test data.\n",
    "\n",
    "We furthermore check how much information about the concept is left after removing information along the directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51792e3d",
   "metadata": {},
   "source": [
    "### User data\n",
    "You need to specify the current working directory and the huggingface [access token](https://huggingface.co/docs/hub/security-tokens) to use this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9527a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify to your current working directory (the directory where this notebook is )\n",
    "cwd = \"exploring_directions\"\n",
    "\n",
    "# enter your authentication token from huggingface and press enter to access the models\n",
    "auth_token = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42edb2e-3928-42cb-9545-fc0d6ef3d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from concept_erasure import LeaceEraser\n",
    "from huggingface_hub import hf_hub_download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2462520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import my modules\n",
    "import sys\n",
    "import importlib\n",
    "# join the path to the modules to the current working directory\n",
    "\n",
    "sys.path.append(os.path.join(cwd, \"modules\"))\n",
    "import wrapping\n",
    "import utils\n",
    "\n",
    "importlib.reload(wrapping)\n",
    "importlib.reload(utils)\n",
    "\n",
    "from wrapping import WrappedModel, WrappedBlock\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6ed473-d090-4fd6-995a-149e110cb92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directories\n",
    "sentiment_data = True\n",
    "use_autoencoder = False\n",
    "sentiment_addon = \"_sentiment\" if sentiment_data else \"\"\n",
    "results_dir =  make_dir(os.path.join(cwd, f'results{sentiment_addon}/'))\n",
    "plots_dir = make_dir(os.path.join(cwd, 'plots{sentiment_addon}'))\n",
    "data_dir = os.path.join(cwd, 'data/ethics/utilitarianism/')\n",
    "\n",
    "model_name = \"pythia-410m-deduped\"\n",
    "model_path = f\"EleutherAI/{model_name}\"\n",
    "precision = torch.float32\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc93c2a6-4104-4657-a0e8-d5d41c542b70",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We show results on the utility dataset. You can download it from [here](https://people.eecs.berkeley.edu/~hendrycks/ethics.tar). Just copy the downloaded folder into the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c588853-5c63-40c1-97e4-671efedd4572",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load_util_data(data_dir=data_dir, split='train')\n",
    "X_test = load_util_data(data_dir=data_dir, split='test')\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "X_train, y_train = mix_util_pairs(X_train)\n",
    "X_test, y_test = mix_util_pairs(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e090077c",
   "metadata": {},
   "source": [
    "### Example sentences\n",
    "The label is one if the first sentence is more utilitarian than the second sentence and zero otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7c18a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(f\"sentence 1: {X_train[i, 0]}\")\n",
    "    print(f\"sentence 2: {X_train[i, 1]}\")\n",
    "    print(f\"  -> label: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffda87fd-ab00-4278-80d9-d7dbbaca1ed2",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90b24dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, use_auth_token=True, device_map=\"auto\").to(device=DEVICE, dtype=precision)\n",
    "model.eval()\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_auth_token=True, device_map=\"auto\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left' \n",
    "\n",
    "batch_size = 128\n",
    "token_pos = -1\n",
    "layer_ids = np.arange(0, model.config.num_hidden_layers, 4)\n",
    "print(layer_ids)\n",
    "wrapped_model = model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865a07ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([\"Yesterday I had a coffee and went to work. Today I am\", \"What I like most about my dog is \", \"I love \", \"I like going to the \"], return_tensors=\"pt\", padding=True, max_length=512, truncation=True).to(device=DEVICE)\n",
    "tokens = model.generate(**inputs, max_new_tokens=50)\n",
    "for i in range(len(tokens)):\n",
    "    print(tokenizer.decode(tokens[i], skip_special_tokens=True))\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b97ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063597d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for wrapped_layer in layer_ids:\n",
    "    wrapped_model.gpt_neox.layers[wrapped_layer] = WrappedBlock(model.gpt_neox.layers[wrapped_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39884f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff9cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1036f01a-213a-432c-bacb-444a01aae8c6",
   "metadata": {},
   "source": [
    "## Get hidden activations\n",
    "We use the custom WrappedModel class to get the internal activations. There are other ways to do this (for example with transformer lens or baukit).\n",
    "We first add a prompt to each sentence to push the model into considering the concept that we aim to extract.\n",
    "We then run each sentence through the model and save the hidden activations in each layer.\n",
    "We get the outputs of the residual stream (the decoder block) per default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe5afa6-ae7b-4e27-9082-1efaf6d46989",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_prompt = 'Consider the amount of happiness in the following scenario:\\n\"{scenario}\"\\nThe amount of happiness in the scenario is '\n",
    "\n",
    "if sentiment_data:\n",
    "    format_prompt = 'Consider if following review is positive or negative:\\n\"{scenario}\"\\nThe review is '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0883f041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(lst, batch_size):\n",
    "    \"\"\"Yield successive batch_size chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield lst[i:i + batch_size]\n",
    "\n",
    "def calc_hidden_states(wrapped_model, sentences, format_prompt, batch_size=128, token_pos=-1, layer_ids=[4]):\n",
    "    hidden_states = {}\n",
    "    for sentence_batch in tqdm(batchify(sentences, batch_size), total=len(sentences)//batch_size):\n",
    "        formatted_sentences = [format_prompt.format_map({'scenario': s}) for s in sentence_batch] \n",
    "               \n",
    "        # get activations\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(formatted_sentences, return_tensors=\"pt\", padding=True, max_length=512, truncation=True)\n",
    "            input_ids = inputs.input_ids.to(DEVICE)\n",
    "            attention_mask = inputs.attention_mask.to(DEVICE)\n",
    "            _ = wrapped_model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # output is a dictionary with layer_ids as keys and the hidden states of the token_pos as values\n",
    "        outputs = {}\n",
    "        for wrapped_layer in layer_ids:\n",
    "            outputs[wrapped_layer] = wrapped_model.gpt_neox.layers[wrapped_layer].output[:, token_pos]\n",
    "            \n",
    "        for key, values in outputs.items():\n",
    "            values = values.detach().float().cpu().numpy()\n",
    "            # Check if the key already exists in hidden_states\n",
    "            if key in hidden_states:\n",
    "                # Concatenate the tensors along axis 0 and update hidden_states\n",
    "                hidden_states[key] = np.concatenate((hidden_states[key], values), axis=0)\n",
    "            else:\n",
    "                # If the key doesn't exist in hidden_states, simply assign the values\n",
    "                hidden_states[key] = values\n",
    "    return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4377355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not sentiment_data:\n",
    "\n",
    "    H0_train = calc_hidden_states(wrapped_model, X_train[:, 0], format_prompt, batch_size=batch_size, token_pos=token_pos, layer_ids=layer_ids)\n",
    "    H1_train = calc_hidden_states(wrapped_model, X_train[:, 1], format_prompt, batch_size=batch_size, token_pos=token_pos, layer_ids=layer_ids)\n",
    "    H0_test = calc_hidden_states(wrapped_model, X_test[:, 0], format_prompt, batch_size=batch_size, token_pos=token_pos, layer_ids=layer_ids)\n",
    "    H1_test = calc_hidden_states(wrapped_model, X_test[:, 1], format_prompt, batch_size=batch_size, token_pos=token_pos, layer_ids=layer_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d816e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sentiment_data:\n",
    "    from datasets import load_dataset\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    token_pos = -1\n",
    "    token_amount= 60\n",
    "    dataset = load_dataset(\"imdb\", split=\"train\").map(\n",
    "        lambda x: {'text': format_prompt.format(scenario=x['text']), 'label': x['label']},\n",
    "    ).map(\n",
    "        lambda x: {'input_ids': tokenizer(x['text'])['input_ids'], 'label': x['label']},\n",
    "        batched=True,\n",
    "    ).filter(\n",
    "        lambda x: len(x['input_ids']) > token_amount\n",
    "    ).map(\n",
    "        lambda x: {'input_ids': x['input_ids'][:token_amount], 'label': x['label']}\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "        dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "        hidden_states = {}\n",
    "        labels = []\n",
    "        for i, batch in enumerate(tqdm(dl)):\n",
    "            _ = wrapped_model(batch.to(DEVICE))\n",
    "            outputs = {}\n",
    "            for wrapped_layer in layer_ids:\n",
    "                outputs[wrapped_layer] = wrapped_model.gpt_neox.layers[wrapped_layer].output[:, token_pos]\n",
    "                \n",
    "            for key, values in outputs.items():\n",
    "                values = values.detach().float().cpu().numpy()\n",
    "                # Check if the key already exists in hidden_states\n",
    "                if key in hidden_states:\n",
    "                    # Concatenate the tensors along axis 0 and update hidden_states\n",
    "                    hidden_states[key] = np.concatenate((hidden_states[key], values), axis=0)\n",
    "                else:\n",
    "                    # If the key doesn't exist in hidden_states, simply assign the values\n",
    "                    hidden_states[key] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974c46e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sentiment_data:\n",
    "    labels = np.array(dataset[\"label\"])\n",
    "    test_split = 0.2\n",
    "    indices = np.arange(len(hidden_states[layer_ids[0]]))\n",
    "\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    test_indices = indices[:int(test_split*len(indices))]\n",
    "    train_indices = indices[int(test_split*len(indices)):]\n",
    "\n",
    "    # make sure that the splits are balanced\n",
    "    positive_train_indices = np.where(labels[train_indices] == 1)[0]\n",
    "    negative_train_indices = np.where(labels[train_indices] == 0)[0]\n",
    "    n_train = min(len(positive_train_indices), len(negative_train_indices))\n",
    "    train_indices_balanced = np.concatenate((train_indices[negative_train_indices[:n_train]], train_indices[positive_train_indices[:n_train]]))\n",
    "\n",
    "    positive_test_indices = np.where(labels[test_indices] == 1)[0]\n",
    "    negative_test_indices = np.where(labels[test_indices] == 0)[0]\n",
    "    n_test = min(len(positive_test_indices), len(negative_test_indices))\n",
    "    test_indices_balanced = np.concatenate((test_indices[negative_test_indices[:n_test]], test_indices[positive_test_indices[:n_test]]))\n",
    "\n",
    "    np.random.shuffle(train_indices_balanced)\n",
    "\n",
    "    H0_train = {key: hidden_states[key][train_indices_balanced] for key in hidden_states.keys()}\n",
    "    H1_train = {key: hidden_states[key][train_indices_balanced] for key in hidden_states.keys()}\n",
    "    H0_test = {key: hidden_states[key][test_indices_balanced] for key in hidden_states.keys()}\n",
    "    H1_test = {key: hidden_states[key][test_indices_balanced] for key in hidden_states.keys()}\n",
    "\n",
    "    y_train = labels[train_indices_balanced]\n",
    "    y_test = labels[test_indices_balanced]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d248e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_autoencoder:\n",
    "    def decode(autoencoder, encoded_hidden_states): \n",
    "\n",
    "        decoded_hidden_states = None\n",
    "        for batch in batchify(encoded_hidden_states, batch_size=128):\n",
    "            values = torch.from_numpy(batch).to(DEVICE)\n",
    "            decoded_hidden_state_batch = autoencoder.decode(values).detach().cpu().numpy()\n",
    "            if decoded_hidden_states is None:\n",
    "                decoded_hidden_states = decoded_hidden_state_batch\n",
    "            else:\n",
    "                decoded_hidden_states = np.concatenate((decoded_hidden_states, decoded_hidden_state_batch), axis=0)\n",
    "\n",
    "        return decoded_hidden_states\n",
    "\n",
    "    def encode(autoencoder, hidden_states):\n",
    "        # encode hidden states\n",
    "        encoded_hidden_states = None\n",
    "        for batch in batchify(hidden_states, batch_size=128):\n",
    "            values = torch.from_numpy(batch).to(DEVICE)\n",
    "            encoded_hidden_state_batch = autoencoder.encode(values).detach().cpu().numpy()\n",
    "            if encoded_hidden_states is None:\n",
    "                encoded_hidden_states = encoded_hidden_state_batch\n",
    "            else:\n",
    "                encoded_hidden_states = np.concatenate((encoded_hidden_states, encoded_hidden_state_batch), axis=0)\n",
    "\n",
    "        return encoded_hidden_states\n",
    "\n",
    "    sys.path.append(\"sparse_coding\")\n",
    "    if model_name == 'pythia-70m-deduped':\n",
    "        layer_ids = np.arange(1, 4)\n",
    "        ending = \"_r6\"\n",
    "    elif model_name == 'pythia-410m-deduped':\n",
    "        layer_ids = [8, 12, 16, 20]\n",
    "        ending = \"_r4\"\n",
    "        \n",
    "    autoencoders = {}\n",
    "    for layer in layer_ids:\n",
    "        ae_download_location = hf_hub_download(repo_id=f\"Elriggs/{model_name}\", filename=f\"tied_residual_l{layer}{ending}/_63/learned_dicts.pt\")\n",
    "        all_autoencoders = torch.load(ae_download_location)\n",
    "        auto_num = 5\n",
    "        autoencoder, hyperparams = all_autoencoders[auto_num]\n",
    "        autoencoder.to_device(DEVICE)\n",
    "\n",
    "        autoencoders[layer] = autoencoder\n",
    "\n",
    "\n",
    "    H0_train_encoded = {}\n",
    "    H1_train_encoded = {}\n",
    "    H0_test_encoded = {}\n",
    "    H1_test_encoded = {}\n",
    "\n",
    "    for layer in tqdm(layer_ids):\n",
    "\n",
    "        H0_train_encoded[layer] = encode(autoencoders[layer], H0_train[layer])\n",
    "        H1_train_encoded[layer] = encode(autoencoders[layer], H1_train[layer])\n",
    "        H0_test_encoded[layer] = encode(autoencoders[layer], H0_test[layer])\n",
    "        H1_test_encoded[layer] = encode(autoencoders[layer], H1_test[layer])\n",
    "\n",
    "    H0_train_decoded = {}\n",
    "    H1_train_decoded = {}\n",
    "    H0_test_decoded = {}\n",
    "    H1_test_decoded = {}\n",
    "\n",
    "    for layer in layer_ids:\n",
    "\n",
    "        H0_train_decoded[layer] = decode(autoencoders[layer], H0_train_encoded[layer])\n",
    "        H1_train_decoded[layer] = decode(autoencoders[layer], H1_train_encoded[layer])\n",
    "        H0_test_decoded[layer] = decode(autoencoders[layer], H0_test_encoded[layer])\n",
    "        H1_test_decoded[layer] = decode(autoencoders[layer], H1_test_encoded[layer])\n",
    "\n",
    "\n",
    "    # print reconstruction error\n",
    "\n",
    "    for layer in layer_ids:\n",
    "        print(f\"layer {layer}\")\n",
    "        print(f\"train reconstruction error: {np.mean((H0_train[layer] - H0_train_decoded[layer])**2)}\")\n",
    "        print(f\"test reconstruction error: {np.mean((H0_test[layer] - H0_test_decoded[layer])**2)}\")\n",
    "        print(\"-\"*30)\n",
    "\n",
    "\n",
    "    # overwrite original data\n",
    "    H0_train = H0_train_encoded\n",
    "    H1_train = H1_train_encoded\n",
    "    H0_test = H0_test_encoded\n",
    "    H1_test = H1_test_encoded\n",
    "\n",
    "    # delete data\n",
    "    del H0_train_decoded\n",
    "    del H1_train_decoded\n",
    "    del H0_test_decoded\n",
    "    del H1_test_decoded\n",
    "    del H0_train_encoded\n",
    "    del H1_train_encoded\n",
    "    del H0_test_encoded\n",
    "    del H1_test_encoded\n",
    "\n",
    "    gc.collect()\n",
    "    model_name = model_name + \"_autoencoder\"\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7983f0d0",
   "metadata": {},
   "source": [
    "We split the training set again, since we want an untouched part of the training set for our removal code.\n",
    "For some methods we use the differences between contrastive pairs. We standardize all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd5470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dir_find = int(H0_train[layer_ids[0]].shape[0]/2)\n",
    "start_clf = n_dir_find\n",
    "\n",
    "H0_train_s, H1_train_s, H_train_s, H_test = {}, {}, {}, {}\n",
    "H0_train_clf, H1_train_clf, H_train_clf = {}, {}, {}\n",
    "y_train_s = y_train[:n_dir_find]\n",
    "y_train_clf = y_train[start_clf:]\n",
    "for layer in H0_train.keys():\n",
    "    H0_train_s[layer], H1_train_s[layer] = H0_train[layer][:n_dir_find], H1_train[layer][:n_dir_find]\n",
    "    H_train_s[layer] = H0_train[layer][:n_dir_find]-H1_train[layer][:n_dir_find]\n",
    "    H0_train_clf[layer], H1_train_clf[layer] = H0_train[layer][start_clf:], H1_train[layer][start_clf:]\n",
    "    H_train_clf[layer] = H0_train[layer][start_clf:]-H1_train[layer][start_clf:]\n",
    "    H_test[layer] = H0_test[layer]-H1_test[layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211510fc-3154-4b93-9d5b-76ba54063fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing data\n",
    "scalers = {}\n",
    "dscalers = {}\n",
    "for layer in layer_ids:\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.concatenate([H0_train_s[layer], H1_train_s[layer]], axis=0))\n",
    "    scalers[layer] = scaler\n",
    "    H0_train_s[layer] = scaler.transform(H0_train_s[layer])\n",
    "    H1_train_s[layer] = scaler.transform(H1_train_s[layer])\n",
    "    H0_train_clf[layer] = scaler.transform(H0_train_clf[layer])\n",
    "    H1_train_clf[layer] = scaler.transform(H1_train_clf[layer])\n",
    "    H0_test[layer] = scaler.transform(H0_test[layer])\n",
    "    H1_test[layer] = scaler.transform(H1_test[layer])\n",
    "\n",
    "    \n",
    "    dscalers[layer] = StandardScaler()\n",
    "    dscalers[layer].fit(H_train_s[layer])\n",
    "    H_train_s[layer] = dscalers[layer].transform(H_train_s[layer])\n",
    "    H_train_clf[layer] = dscalers[layer].transform(H_train_clf[layer])\n",
    "    H_test[layer] = dscalers[layer].transform(H_test[layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a7e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sentiment_data:\n",
    "    H_train_s = H0_train_s\n",
    "    H_train_clf = H0_train_clf\n",
    "    H_test = H0_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22062fd8-440c-4ed9-963e-130b8aa32723",
   "metadata": {},
   "source": [
    "# Finding directions using different methods\n",
    "\n",
    "We find the directions using the hidden representation of our formatted sentences directly or after taking differences between contrastive pairs. To ensure that all directions point towards positive utility we project the training data on the un oriented direction and find the correct coefficient for the orientation using logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae17da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723aee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA on differences of contrastive pairs\n",
    "directions[\"PCA_diffs\"] = {}\n",
    "for layer in tqdm(layer_ids):\n",
    "    pca = PCA(n_components=1, whiten=True).fit(H_train_s[layer])\n",
    "    direction = pca.components_.squeeze()\n",
    "    temp = pca.transform(H_train_s[layer])\n",
    "    lr = LogisticRegression(solver='liblinear').fit(temp, y_train_s)\n",
    "    coeff = np.sign(lr.coef_).squeeze()\n",
    "    directions[\"PCA_diffs\"][layer] = coeff*direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658c2367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class means\n",
    "directions[\"ClassMeans\"] = {}\n",
    "for layer in tqdm(layer_ids):\n",
    "    direction = H_train_s[layer][y_train_s==1].mean(axis=0) - H_train_s[layer][y_train_s==0].mean(axis=0)\n",
    "    directions[\"ClassMeans\"][layer] = direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104eeb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "directions[\"LogReg\"] = {}\n",
    "for layer in tqdm(layer_ids):\n",
    "    lr = LogisticRegression(solver='liblinear', C=1e-2, random_state=0, max_iter=50).fit(H_train_clf[layer], y_train_clf)\n",
    "    directions[\"LogReg\"][layer] = lr.coef_.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73bf17-3ca3-4b2e-8885-0455b3942497",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.join(results_dir, f'directions_{model_name}.pkl')\n",
    "with open(fname, 'wb') as f:\n",
    "    pickle.dump(directions, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fb094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.join(results_dir, f'directions_{model_name}.pkl')\n",
    "with open(fname, 'rb') as f:\n",
    "    directions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3b6499",
   "metadata": {},
   "source": [
    "### Cosine similarity between directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d844bb-eadf-41ee-9a7d-6d1b159c548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "# calculate cosine similarity between directions\n",
    "for layer in tqdm(layer_ids):\n",
    "    temp = {key : directions[key][layer] for key in directions.keys()}\n",
    "    for key in temp.keys():\n",
    "        temp[key] = temp[key].squeeze()\n",
    "    df = pd.DataFrame.from_dict(temp, orient='index')\n",
    "    cosine_sim_matrix = cosine_similarity(df.values)\n",
    "    cosine_sim_df = pd.DataFrame(cosine_sim_matrix, index=df.index, columns=df.index)\n",
    "    dfs.append(cosine_sim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfd29a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 3\n",
    "sns.heatmap(dfs[layer], annot=True)\n",
    "plt.savefig(os.path.join(plots_dir, f'cos_similarity_{model_name}_layer_{layer}.png'), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6150446d-f260-44c0-9e72-c21433a04cb1",
   "metadata": {},
   "source": [
    "# Classification - Test for Correlation\n",
    "How well can the found directions separate the data? We test on differences of the hidden representation of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5af504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(H, y, direction):\n",
    "    Hte_i = np.dot(H, direction.T)\n",
    "    accte = ((Hte_i > 0) == y).sum()/len(y)\n",
    "    return accte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a2ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "directions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d58a47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "separation_test_accs = {}\n",
    "separation_train_accs = {}\n",
    "\n",
    "for method in tqdm(directions.keys()):\n",
    "    separation_test_accs[method] = {}\n",
    "    separation_train_accs[method] = {}\n",
    "    for layer in layer_ids:\n",
    "        if method == \"Random\":\n",
    "            temp = 0\n",
    "            temp_train = 0\n",
    "            random_runs = directions[method][layer].shape[0]\n",
    "            for i in range(random_runs):\n",
    "                temp += classification(H_test[layer], y_test, directions[method][layer][i])\n",
    "                temp_train += classification(H_train_s[layer], y_train_s, directions[method][layer][i])\n",
    "            separation_test_accs[method][layer] = temp/random_runs\n",
    "            separation_train_accs[method][layer] = temp_train/random_runs\n",
    "        else:\n",
    "            separation_test_accs[method][layer] = classification(H_test[layer], y_test, directions[method][layer])\n",
    "            separation_train_accs[method][layer] = classification(H_train_s[layer], y_train_s, directions[method][layer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f410c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lines(separation_train_accs, \"train accuracy\", save_path=os.path.join(plots_dir, f'separation_train_accs_{model_name}.png'), method_names=directions.keys(), loc='center right')\n",
    "plot_lines(separation_test_accs, \"test accuracy\", save_path=os.path.join(plots_dir, f'separation_test_accs_{model_name}.png'), method_names=directions.keys(), loc='center right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bf2dcd",
   "metadata": {},
   "source": [
    "# Erasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92de0c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project onto hyperplane perpendicular to direction\n",
    "def project(activation, direction):\n",
    "    dot_product = np.dot(activation, direction)\n",
    "    squared_norm = np.sum(direction ** 2)\n",
    "    projection = np.outer(dot_product / squared_norm, direction)\n",
    "    output = activation - projection\n",
    "    return output     \n",
    "\n",
    "# linear regression\n",
    "def linear_regr(H_trainl, H_testl, y_train, y_test):\n",
    "    # scalerem = StandardScaler()\n",
    "    # H_trainl = scalerem.fit_transform(H_trainl)\n",
    "    # H_testl = scalerem.transform(H_testl)\n",
    "    clf = LogisticRegression(solver='liblinear', C=1e-2, random_state=0, max_iter=50).fit(H_trainl, y_train)\n",
    "    clf.intercept_ = 0\n",
    "    acctr = clf.score(H_trainl, y_train)\n",
    "    accte = clf.score(H_testl, y_test)\n",
    "\n",
    "    acctr = classification(H_trainl, y_train, clf.coef_.squeeze())\n",
    "    accte = classification(H_testl, y_test, clf.coef_.squeeze())\n",
    "    return acctr, accte\n",
    "\n",
    "def erase_concept(H_train, H_test, y_train, y_test, direction=None, layers=list(range(model.config.num_hidden_layers))):\n",
    "    train_acc_l, test_acc_l = {}, {}\n",
    "    for layer in tqdm(layers): \n",
    "        if direction is not None:\n",
    "            if len(direction[layer].shape) > 1:\n",
    "                train_acc_temp, test_acc_temp = 0.0, 0.0\n",
    "                for i in range(direction[layer].shape[0]):\n",
    "                    H_train_l = project(H_train[layer], direction[layer][i])\n",
    "                    H_test_l = project(H_test[layer], direction[layer][i])\n",
    "                    acctr, accte = linear_regr(H_train_l, H_test_l, y_train, y_test)\n",
    "                    train_acc_temp += acctr\n",
    "                    test_acc_temp += accte\n",
    "                train_acc_l[layer]= train_acc_temp/direction[layer].shape[0]\n",
    "                test_acc_l[layer] = test_acc_temp/direction[layer].shape[0]\n",
    "            else:\n",
    "                H_train_l = project(H_train[layer], direction[layer])\n",
    "                H_test_l = project(H_test[layer], direction[layer])\n",
    "                acctr, accte = linear_regr(H_train_l, H_test_l, y_train, y_test)\n",
    "                train_acc_l[layer]= acctr\n",
    "                test_acc_l[layer] = accte\n",
    "\n",
    "        else:\n",
    "            acctr, accte = linear_regr(H_train[layer], H_test[layer], y_train, y_test)\n",
    "            train_acc_l[layer]= acctr\n",
    "            test_acc_l[layer] = accte\n",
    "            \n",
    "    return train_acc_l, test_acc_l\n",
    "    \n",
    "def leace(H_train_s, y_train_s, H_train_clf, y_train_clf, H_test, y_test, layers=list(range(model.config.num_hidden_layers))):\n",
    "    train_acc_l, test_acc_l = {}, {}\n",
    "    for layer in tqdm(layers): \n",
    "        H_trainl, H_train_clfl, H_testl = H_train_s[layer], H_train_clf[layer], H_test[layer]\n",
    "        # eraser is trained on first half of training set\n",
    "        eraser = LeaceEraser.fit(torch.from_numpy(H_trainl), torch.from_numpy(y_train_s))\n",
    "        # erase from second half of training set and test set\n",
    "        H_train_clf_tch = eraser(torch.from_numpy(H_train_clfl))\n",
    "        H_test_tch = eraser(torch.from_numpy(H_testl))\n",
    "        H_train_clfl, H_testl = torch.Tensor.numpy(H_train_clf_tch), torch.Tensor.numpy(H_test_tch)\n",
    "        \n",
    "        acctr, accte = linear_regr(H_train_clfl, H_testl, y_train_clf, y_test)\n",
    "        train_acc_l[layer] = acctr\n",
    "        test_acc_l[layer] = accte\n",
    "            \n",
    "    return train_acc_l, test_acc_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe4dabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accs, test_accs = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92699033",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accs, test_accs = {}, {}\n",
    "\n",
    "for key in directions.keys():\n",
    "    print(key)\n",
    "    train_accs[key], test_accs[key] = erase_concept(H_train_clf, H_test, y_train_clf, y_test, directions[key], layers=layer_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3157128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline without erasure\n",
    "train_accs[\"NoErasure\"], test_accs[\"NoErasure\"] = erase_concept(H_train_s, H_test, y_train_s, y_test, layers=layer_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99f622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# erase with leace\n",
    "train_accs[\"LEACE\"], test_accs[\"LEACE\"] = leace(H_train_s, y_train_s, H_train_clf, y_train_clf, H_test, y_test, layers=layer_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64620040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "fname = os.path.join(results_dir, f'removal_{model_name}.pkl')\n",
    "with open(fname, 'wb') as f:\n",
    "    pickle.dump([train_accs, test_accs], f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c670a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "fname = os.path.join(results_dir, f'removal_{model_name}.pkl')\n",
    "with open(fname, 'rb') as f:\n",
    "    train_accs, test_accs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38912cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plot_lines(train_accs, \"Train accuracy\", save_path=os.path.join(plots_dir, f'removal_train_accs_{model_name}.png'), method_names=train_accs.keys(), loc='lower right')\n",
    "plot_lines(test_accs, \"Test accuracy\", save_path=os.path.join(plots_dir, f'removal_test_accs_{model_name}.png'), method_names=test_accs.keys(), loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75f8a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_sentences = 100\n",
    "generations_dir = make_dir(os.path.join(results_dir, 'generations_pythia/'))\n",
    "\n",
    "data_dir = make_dir(os.path.join(cwd, 'data/'))\n",
    "\n",
    "data_file = os.path.join(data_dir, 'test_sentences.txt')\n",
    "random_seed = 42\n",
    "max_new_tokens = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77876331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data\n",
    "X_test = load_util_data(data_dir=os.path.join(cwd, 'data/ethics/utilitarianism/'), split='test')\n",
    "X_test, y_test = mix_util_pairs(X_test)\n",
    "\n",
    "test_data_idxs, test_sentences = find_two_sentences(X_test[:, 0], split_str1=\".\", split_str2=\",\", larger_than1=2, larger_than2=1)\n",
    "\n",
    "with open(data_file, \"w\") as f:\n",
    "    for s in test_sentences:\n",
    "        f.write(s + \" \\n\")\n",
    "\n",
    "# load data\n",
    "with open(data_file, 'r') as f:\n",
    "    test_sentences = [line.strip() for line in f]\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "idxs = np.random.choice(len(test_sentences), num_test_sentences, replace=False)\n",
    "test_sentences = [test_sentences[idx] for idx in idxs]\n",
    "for i in range(10):\n",
    "    print(test_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df38300",
   "metadata": {},
   "outputs": [],
   "source": [
    "generations = []\n",
    "for sentence_batch in batchify(test_sentences, batch_size):\n",
    "    with torch.no_grad():\n",
    "        torch.random.manual_seed(random_seed)\n",
    "        inputs = tokenizer(sentence_batch, return_tensors=\"pt\", padding=True, max_length=512, truncation=True)\n",
    "        attention_mask = inputs.attention_mask.to(DEVICE)\n",
    "        generate_ids = wrapped_model.generate(inputs.input_ids.to(DEVICE), attention_mask=attention_mask, max_new_tokens=max_new_tokens, use_cache=True)\n",
    "        generated = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "    generations.extend(generated)\n",
    "\n",
    "# save generations as text files\n",
    "with open(os.path.join(generations_dir, f\"generations_neutral_{model_name}.txt\"), \"w\") as f:\n",
    "    for item in generations:\n",
    "        # remove newline characters\n",
    "        item = item.replace(\"\\n\", \" \")\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a255c47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
