{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1306b6e0",
   "metadata": {},
   "source": [
    "# Extracting directions, Classification and Removal\n",
    "We search for directions corresponding to a concept in hidden layer activations.\n",
    "We use several different methods:\n",
    "* One Prompt\n",
    "* Logistic Regression\n",
    "* Principal Component Analysis (PCA)\n",
    "* Class Means\n",
    "* K-Means\n",
    "* Random Direction as a baseline\n",
    "\n",
    "We check how well the directions correlate with the concept we care about by using them to separate the test data.\n",
    "\n",
    "We furthermore check how much information about the concept is left after removing information along the directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42edb2e-3928-42cb-9545-fc0d6ef3d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42076df",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = \"/data/ann_kathrin_dombrowski/ICE/ice_baseline/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2462520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import my modules\n",
    "\n",
    "# Append the absolute path to sys.path\n",
    "sys.path.append(os.path.join(my_path, 'modules'))\n",
    "import wrapping\n",
    "import utils\n",
    "importlib.reload(wrapping)\n",
    "importlib.reload(utils)\n",
    "from wrapping import WrappedModel\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6ed473-d090-4fd6-995a-149e110cb92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = f'{my_path}results/'\n",
    "plots_dir = f'{my_path}plots/'\n",
    "data_dir = f'/data/ann_kathrin_dombrowski/ethics/utilitarianism/' # download from https://github.com/hendrycks/ethics\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc93c2a6-4104-4657-a0e8-d5d41c542b70",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We show results on the utility dataset. You can download it from [here](https://people.eecs.berkeley.edu/~hendrycks/ethics.tar). Just copy the downloaded folder into the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c588853-5c63-40c1-97e4-671efedd4572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_util_sentences(split=\"test\"):\n",
    "    data = pd.read_csv(data_dir + f'util_{split}.csv', header=None)\n",
    "    sentences = []\n",
    "    for d in data.iloc:\n",
    "        sentences.append([d[0], d[1]])\n",
    "    return np.array(sentences)\n",
    "\n",
    "\n",
    "\n",
    "X_train = load_util_sentences(split='train')\n",
    "X_test = load_util_sentences(split='test')\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "# per default always the first sentence is the sentence with higher utility\n",
    "# this function swaps the sentences with probability 0.5\n",
    "def get_data(rng, data):\n",
    "    m = len(data)\n",
    "    labels = rng.integers(0, 2, size=m)\n",
    "    data = data[np.arange(m), [1-labels, labels]].T\n",
    "    return data, labels\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "X_train, y_train = get_data(rng, X_train)\n",
    "X_test, y_test = get_data(rng, X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e090077c",
   "metadata": {},
   "source": [
    "### Example sentences\n",
    "The label is one if the first sentence is more utilitarian than the second sentence and zero otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7c18a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(f\"sentence 1: {X_train[i, 0]}\")\n",
    "    print(f\"sentence 2: {X_train[i, 1]}\")\n",
    "    print(f\"  -> label: {y_train[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffda87fd-ab00-4278-80d9-d7dbbaca1ed2",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "Use a huggingface [access token](https://huggingface.co/docs/hub/security-tokens) and load the Llama-2-7b-chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8addfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your authentication token from huggingface and press enter to access the models\n",
    "auth_token = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90b24dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_name = \"Llama-2-7b-chat-hf\"\n",
    "model_path = f\"meta-llama/{model_name}\"\n",
    "precision = torch.bfloat16\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, token=auth_token, device_map=\"auto\").to(device=DEVICE, dtype=precision)\n",
    "model.eval()\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, token=auth_token, device_map=\"auto\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d465db-acfb-4d90-b976-d1d03751f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = model.config.num_hidden_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1036f01a-213a-432c-bacb-444a01aae8c6",
   "metadata": {},
   "source": [
    "## Get hidden activations\n",
    "We use the custom WrappedModel class to get the internal activations. There are other ways to do this (for example with transformer lens or baukit).\n",
    "We first add a prompt to each sentence to push the model into considering the concept that we aim to extract.\n",
    "We then run each sentence through the model and save the hidden activations in each layer.\n",
    "We get the outputs of the residual stream (the decoder block) per default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f423eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = model.config.num_hidden_layers\n",
    "layer_ids = np.arange(0, num_layers) # which layers do we want to extract the hidden states from?\n",
    "\n",
    "hidden_dim = model.config.hidden_size\n",
    "block_name = \"decoder_block\" # should be one of ['self_attn', 'mlp', 'input_layernorm', 'post_attention_layernorm', 'decoder_block']\n",
    "token_pos = -1 # at which token do we want to extract the hidden states? -1 means the last token.\n",
    "batch_size = 128\n",
    "# WRAP MODEL\n",
    "# wrap the model in a class that allows to access the hidden states\n",
    "wrapped_model = WrappedModel(model, tokenizer)\n",
    "# make sure nothing is wrapped from previous runs\n",
    "wrapped_model.unwrap()\n",
    "# wrap the block you want to wrap\n",
    "wrapped_model.wrap_block(layer_ids, block_name=block_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe5afa6-ae7b-4e27-9082-1efaf6d46989",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_prompt = 'Consider the amount of happiness in the following scenario:\\n\"{scenario}\"\\nThe amount of happiness in the scenario is '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0883f041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(lst, batch_size):\n",
    "    \"\"\"Yield successive batch_size chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield lst[i:i + batch_size]\n",
    "\n",
    "def calc_hidden_states(wrapped_model, sentences, format_prompt, batch_size=128, token_pos=-1):\n",
    "    hidden_states = {}\n",
    "    for sentence_batch in tqdm(batchify(sentences, batch_size), total=len(sentences)//batch_size):\n",
    "        wrapped_model.reset()\n",
    "        gc.collect()\n",
    "\n",
    "        formatted_sentences = [format_prompt.format_map({'scenario': s}) for s in sentence_batch]        \n",
    "        # get activations\n",
    "        _ = wrapped_model.run_prompt(formatted_sentences) # this saves the hidden states in the wrapped_model object\n",
    "        # output is a dictionary with layer_ids as keys and the hidden states of the token_pos as values\n",
    "        outputs = wrapped_model.get_activations(layer_ids, block_name=block_name, token_pos=token_pos)\n",
    "        for key, values in outputs.items():\n",
    "            values = values.detach().float().cpu().numpy()\n",
    "            # Check if the key already exists in hidden_states\n",
    "            if key in hidden_states:\n",
    "                # Concatenate the tensors along axis 0 and update hidden_states\n",
    "                hidden_states[key] = np.concatenate((hidden_states[key], values), axis=0)\n",
    "            else:\n",
    "                # If the key doesn't exist in hidden_states, simply assign the values\n",
    "                hidden_states[key] = values\n",
    "    return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4377355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "H0_train = calc_hidden_states(wrapped_model, X_train[:, 0], format_prompt, batch_size=batch_size, token_pos=token_pos)\n",
    "H1_train = calc_hidden_states(wrapped_model, X_train[:, 1], format_prompt, batch_size=batch_size, token_pos=token_pos)\n",
    "H0_test = calc_hidden_states(wrapped_model, X_test[:, 0], format_prompt, batch_size=batch_size, token_pos=token_pos)\n",
    "H1_test = calc_hidden_states(wrapped_model, X_test[:, 1], format_prompt, batch_size=batch_size, token_pos=token_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7983f0d0",
   "metadata": {},
   "source": [
    "We split the training set again, since we want an untouched part of the training set for our removal code.\n",
    "For some methods we use the differences between contrastive pairs. We normalize all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd5470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dir_find = int(H0_train[0].shape[0]/2)\n",
    "start_clf = n_dir_find\n",
    "# n_dir_find = 2500\n",
    "\n",
    "H0_train_s, H1_train_s, H_train_s, H_test = {}, {}, {}, {}\n",
    "H0_train_clf, H1_train_clf, H_train_clf = {}, {}, {}\n",
    "y_train_s = y_train[:n_dir_find]\n",
    "y_train_clf = y_train[start_clf:]\n",
    "for layer in H0_train.keys():\n",
    "    H0_train_s[layer], H1_train_s[layer] = H0_train[layer][:n_dir_find], H1_train[layer][:n_dir_find]\n",
    "    H_train_s[layer] = H0_train[layer][:n_dir_find]-H1_train[layer][:n_dir_find]\n",
    "    H0_train_clf[layer], H1_train_clf[layer] = H0_train[layer][start_clf:], H1_train[layer][start_clf:]\n",
    "    H_train_clf[layer] = H0_train[layer][start_clf:]-H1_train[layer][start_clf:]\n",
    "    H_test[layer] = H0_test[layer]-H1_test[layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211510fc-3154-4b93-9d5b-76ba54063fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing data\n",
    "scalers = {}\n",
    "dscalers = {}\n",
    "for layer in layer_ids:\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.concatenate([H0_train_s[layer], H1_train_s[layer]], axis=0))\n",
    "    scalers[layer] = scaler\n",
    "    H0_train_s[layer] = scaler.transform(H0_train_s[layer])\n",
    "    H1_train_s[layer] = scaler.transform(H1_train_s[layer])\n",
    "    H0_train_clf[layer] = scaler.transform(H0_train_clf[layer])\n",
    "    H1_train_clf[layer] = scaler.transform(H1_train_clf[layer])\n",
    "    H0_test[layer] = scaler.transform(H0_test[layer])\n",
    "    H1_test[layer] = scaler.transform(H1_test[layer])\n",
    "\n",
    "    \n",
    "    dscalers[layer] = StandardScaler()\n",
    "    dscalers[layer].fit(H_train_s[layer])\n",
    "    H_train_s[layer] = dscalers[layer].transform(H_train_s[layer])\n",
    "    H_train_clf[layer] = dscalers[layer].transform(H_train_clf[layer])\n",
    "    H_test[layer] = dscalers[layer].transform(H_test[layer])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22062fd8-440c-4ed9-963e-130b8aa32723",
   "metadata": {},
   "source": [
    "# Finding directions using different methods\n",
    "\n",
    "We find the directions using the hidden representation of our formatted sentences directly or after taking differences between contrastive pairs. To ensure that all directions point towards positive utility we project the training data on the un oriented direction and find the correct coefficient for the orientation using logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae17da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496f7af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one prompt method\n",
    "\n",
    "H0_prompt = calc_hidden_states(wrapped_model, [\"Love\"], format_prompt, batch_size=batch_size, token_pos=token_pos)\n",
    "H1_prompt = calc_hidden_states(wrapped_model, [\"Hate\"], format_prompt, batch_size=batch_size, token_pos=token_pos)\n",
    "\n",
    "directions[\"OnePrompt\"] = {}\n",
    "\n",
    "for layer in tqdm(layer_ids):\n",
    "    H0_prompt[layer] = scalers[layer].transform(H0_prompt[layer])\n",
    "    H1_prompt[layer] = scalers[layer].transform(H1_prompt[layer])\n",
    "    direction = H0_prompt[layer]-H1_prompt[layer]\n",
    "    direction = dscalers[layer].transform(direction)\n",
    "    directions[\"OnePrompt\"][layer] = direction.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5344058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "directions[\"OnePrompt\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1009857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random method\n",
    "directions[\"Random\"] = {}\n",
    "num_random_directions = 10\n",
    "for layer in tqdm(layer_ids):\n",
    "    direction = np.random.normal(0.0, 1.0, size=(num_random_directions, hidden_dim))\n",
    "    for i in range(direction.shape[0]):\n",
    "        # project data onto direction\n",
    "        Htr_i = np.dot(H_train_s[layer], direction[i].squeeze().T)\n",
    "        lr = LogisticRegression(solver='liblinear').fit(Htr_i.reshape(-1, 1), y_train_s)\n",
    "        coeff = np.sign(lr.coef_).squeeze()\n",
    "        direction[i] = coeff*direction[i]\n",
    "    directions[\"Random\"][layer] = direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62976d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "directions[\"LogReg\"] = {}\n",
    "for layer in tqdm(layer_ids):\n",
    "    lr = LogisticRegression(solver='liblinear', C=1e-2, random_state=0, max_iter=50).fit(H_train_s[layer], y_train_s)\n",
    "    coeff = np.sign(lr.coef_).squeeze()\n",
    "    directions[\"LogReg\"][layer] = coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be654b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class means\n",
    "directions[\"ClassMeans\"] = {}\n",
    "for layer in tqdm(layer_ids):\n",
    "    direction = H_train_s[layer][y_train_s==1].mean(axis=0) - H_train_s[layer][y_train_s==0].mean(axis=0)\n",
    "    directions[\"ClassMeans\"][layer] = direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723aee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA on differences of contrastive pairs\n",
    "directions[\"PCA_diffs\"] = {}\n",
    "for layer in tqdm(layer_ids):\n",
    "    pca = PCA(n_components=1, whiten=True).fit(H_train_s[layer])\n",
    "    direction = pca.components_.squeeze()\n",
    "    temp = pca.transform(H_train_s[layer])\n",
    "    lr = LogisticRegression(solver='liblinear').fit(temp, y_train_s)\n",
    "    coeff = np.sign(lr.coef_).squeeze()\n",
    "    directions[\"PCA_diffs\"][layer] = coeff*direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d7ac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA without taking differences\n",
    "directions[\"PCA\"] = {}\n",
    "for layer in tqdm(layer_ids):\n",
    "    pca = PCA(n_components=1, whiten=True).fit(np.concatenate([H0_train_s[layer], H1_train_s[layer]], axis=0))\n",
    "    direction = pca.components_.squeeze()\n",
    "    temp = pca.transform(H_train_s[layer])\n",
    "    lr = LogisticRegression(solver='liblinear').fit(temp, y_train_s)\n",
    "    coeff = np.sign(lr.coef_).squeeze()\n",
    "    directions[\"PCA\"][layer] = coeff*direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e516ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means\n",
    "directions[\"K-Means\"] = {}\n",
    "for layer in tqdm(layer_ids):\n",
    "    kmeans = KMeans(n_clusters=2, n_init=10).fit(np.concatenate([H0_train_s[layer], H1_train_s[layer]], axis=0))\n",
    "    direction = kmeans.cluster_centers_[0] - kmeans.cluster_centers_[1]\n",
    "    # project onto direction\n",
    "    temp = np.dot(H_train_s[layer], direction.squeeze().T)\n",
    "    lr = LogisticRegression(solver='liblinear').fit(temp.reshape(-1, 1), y_train_s)\n",
    "    coeff = np.sign(lr.coef_).squeeze()\n",
    "    directions[\"K-Means\"][layer] = direction*coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73bf17-3ca3-4b2e-8885-0455b3942497",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = results_dir + 'utility_directions_{model_name}.pkl'\n",
    "with open(fname, 'wb') as f:\n",
    "    pickle.dump(directions, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3b6499",
   "metadata": {},
   "source": [
    "### Cosine similarity between directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d844bb-eadf-41ee-9a7d-6d1b159c548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "# calculate cosine similarity between directions\n",
    "for layer in tqdm(range(num_layers)):\n",
    "    temp = {key : directions[key][layer] for key in directions.keys()}\n",
    "    temp.pop(\"Random\")\n",
    "    for key in temp.keys():\n",
    "        temp[key] = temp[key].squeeze()\n",
    "    df = pd.DataFrame.from_dict(temp, orient='index')\n",
    "    cosine_sim_matrix = cosine_similarity(df.values)\n",
    "    cosine_sim_df = pd.DataFrame(cosine_sim_matrix, index=df.index, columns=df.index)\n",
    "    dfs.append(cosine_sim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc25d64-1a42-4f6e-a655-6de7e4ad338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(dfs[20], annot=True)\n",
    "plt.savefig(plots_dir + f'utility_cossine_{model_name}_selection.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6150446d-f260-44c0-9e72-c21433a04cb1",
   "metadata": {},
   "source": [
    "# Classification - Test for Correlation\n",
    "How well can the found directions separate the data? We test on differences of the hidden representation of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5af504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(H, y, direction):\n",
    "    Hte_i = np.dot(H, direction.T)\n",
    "    accte = ((Hte_i > 0) == y).sum()/len(y)\n",
    "    return accte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d58a47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accs = {}\n",
    "for method in tqdm(directions.keys()):\n",
    "    test_accs[method] = {}\n",
    "    for layer in layer_ids:\n",
    "        if method == \"Random\":\n",
    "            temp = 0\n",
    "            random_runs = directions[method][layer].shape[0]\n",
    "            for i in range(random_runs):\n",
    "                temp += classification(H_test[layer], y_test, directions[method][layer][i])\n",
    "            test_accs[method][layer] = temp/random_runs\n",
    "        else:\n",
    "            test_accs[method][layer] = classification(H_test[layer], y_test, directions[method][layer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f410c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lines(test_accs, \"test accuracy\", save_path=plots_dir + f'baselines_utility_{model_name}.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
