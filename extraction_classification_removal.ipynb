{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1306b6e0",
   "metadata": {},
   "source": [
    "# Extracting directions, Classification and Removal\n",
    "We search for directions corresponding to a concept in hidden layer activations.\n",
    "We use several different methods:\n",
    "* One Prompt\n",
    "* Logistic Regression\n",
    "* Principal Component Analysis (PCA)\n",
    "* Class Means\n",
    "* K-Means\n",
    "* Random Direction as a baseline\n",
    "\n",
    "We check how well the directions correlate with the concept we care about by using them to separate the test data.\n",
    "\n",
    "We furthermore check how much information about the concept is left after removing information along the directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51792e3d",
   "metadata": {},
   "source": [
    "### User data\n",
    "You need to specify the current working directory and the huggingface [access token](https://huggingface.co/docs/hub/security-tokens) to use this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9527a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify to your current working directory (the directory where this notebook is )\n",
    "cwd = \"exploring_directions\"\n",
    "\n",
    "# enter your authentication token from huggingface and press enter to access the models\n",
    "auth_token = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42edb2e-3928-42cb-9545-fc0d6ef3d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from concept_erasure import LeaceEraser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2462520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import my modules\n",
    "import sys\n",
    "import importlib\n",
    "# join the path to the modules to the current working directory\n",
    "\n",
    "sys.path.append(os.path.join(cwd, \"modules\"))\n",
    "import wrapping\n",
    "import utils\n",
    "\n",
    "importlib.reload(wrapping)\n",
    "importlib.reload(utils)\n",
    "\n",
    "from wrapping import WrappedModel\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6ed473-d090-4fd6-995a-149e110cb92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directories\n",
    "results_dir =  make_dir(os.path.join(cwd, 'results/'))\n",
    "plots_dir = make_dir(os.path.join(cwd, 'plots'))\n",
    "data_dir = os.path.join(cwd, 'data/ethics/utilitarianism/')\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc93c2a6-4104-4657-a0e8-d5d41c542b70",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We show results on the utility dataset. You can download it from [here](https://people.eecs.berkeley.edu/~hendrycks/ethics.tar). Just copy the downloaded folder into the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c588853-5c63-40c1-97e4-671efedd4572",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load_util_data(data_dir=data_dir, split='train')\n",
    "X_test = load_util_data(data_dir=data_dir,split='test')\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "X_train, y_train = mix_util_pairs(X_train)\n",
    "X_test, y_test = mix_util_pairs(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e090077c",
   "metadata": {},
   "source": [
    "### Example sentences\n",
    "The label is one if the first sentence is more utilitarian than the second sentence and zero otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7c18a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(f\"sentence 1: {X_train[i, 0]}\")\n",
    "    print(f\"sentence 2: {X_train[i, 1]}\")\n",
    "    print(f\"  -> label: {y_train[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffda87fd-ab00-4278-80d9-d7dbbaca1ed2",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90b24dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_name = \"Llama-2-7b-chat-hf\"\n",
    "model_path = f\"meta-llama/{model_name}\"\n",
    "precision = torch.bfloat16\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, token=auth_token, device_map=\"auto\").to(device=DEVICE, dtype=precision)\n",
    "model.eval()\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, token=auth_token, device_map=\"auto\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d465db-acfb-4d90-b976-d1d03751f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = model.config.num_hidden_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1036f01a-213a-432c-bacb-444a01aae8c6",
   "metadata": {},
   "source": [
    "## Get hidden activations\n",
    "We use the custom WrappedModel class to get the internal activations. There are other ways to do this (for example with transformer lens or baukit).\n",
    "We first add a prompt to each sentence to push the model into considering the concept that we aim to extract.\n",
    "We then run each sentence through the model and save the hidden activations in each layer.\n",
    "We get the outputs of the residual stream (the decoder block) per default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f423eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = model.config.num_hidden_layers\n",
    "layer_ids = np.arange(0, num_layers) # which layers do we want to extract the hidden states from?\n",
    "\n",
    "hidden_dim = model.config.hidden_size\n",
    "block_name = \"decoder_block\" # should be one of ['self_attn', 'mlp', 'input_layernorm', 'post_attention_layernorm', 'decoder_block']\n",
    "token_pos = -1 # at which token do we want to extract the hidden states? -1 means the last token.\n",
    "batch_size = 128\n",
    "# WRAP MODEL\n",
    "# wrap the model in a class that allows to access the hidden states\n",
    "wrapped_model = WrappedModel(model, tokenizer)\n",
    "# make sure nothing is wrapped from previous runs\n",
    "wrapped_model.unwrap()\n",
    "# wrap the block you want to wrap\n",
    "wrapped_model.wrap_block(layer_ids, block_name=block_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe5afa6-ae7b-4e27-9082-1efaf6d46989",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_prompt = 'Consider the amount of happiness in the following scenario:\\n\"{scenario}\"\\nThe amount of happiness in the scenario is '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0883f041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(lst, batch_size):\n",
    "    \"\"\"Yield successive batch_size chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield lst[i:i + batch_size]\n",
    "\n",
    "def calc_hidden_states(wrapped_model, sentences, format_prompt, batch_size=128, token_pos=-1):\n",
    "    hidden_states = {}\n",
    "    for sentence_batch in tqdm(batchify(sentences, batch_size), total=len(sentences)//batch_size):\n",
    "        wrapped_model.reset()\n",
    "        gc.collect()\n",
    "\n",
    "        formatted_sentences = [format_prompt.format_map({'scenario': s}) for s in sentence_batch]        \n",
    "        # get activations\n",
    "        _ = wrapped_model.run_prompt(formatted_sentences) # this saves the hidden states in the wrapped_model object\n",
    "        # output is a dictionary with layer_ids as keys and the hidden states of the token_pos as values\n",
    "        outputs = wrapped_model.get_activations(layer_ids, block_name=block_name, token_pos=token_pos)\n",
    "        for key, values in outputs.items():\n",
    "            values = values.detach().float().cpu().numpy()\n",
    "            # Check if the key already exists in hidden_states\n",
    "            if key in hidden_states:\n",
    "                # Concatenate the tensors along axis 0 and update hidden_states\n",
    "                hidden_states[key] = np.concatenate((hidden_states[key], values), axis=0)\n",
    "            else:\n",
    "                # If the key doesn't exist in hidden_states, simply assign the values\n",
    "                hidden_states[key] = values\n",
    "    return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4377355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "H0_train = calc_hidden_states(wrapped_model, X_train[:, 0], format_prompt, batch_size=batch_size, token_pos=token_pos)\n",
    "H1_train = calc_hidden_states(wrapped_model, X_train[:, 1], format_prompt, batch_size=batch_size, token_pos=token_pos)\n",
    "H0_test = calc_hidden_states(wrapped_model, X_test[:, 0], format_prompt, batch_size=batch_size, token_pos=token_pos)\n",
    "H1_test = calc_hidden_states(wrapped_model, X_test[:, 1], format_prompt, batch_size=batch_size, token_pos=token_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7983f0d0",
   "metadata": {},
   "source": [
    "We split the training set again, since we want an untouched part of the training set for our removal code.\n",
    "For some methods we use the differences between contrastive pairs. We standardize all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd5470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dir_find = int(H0_train[0].shape[0]/2)\n",
    "start_clf = n_dir_find\n",
    "# n_dir_find = 2500\n",
    "\n",
    "H0_train_s, H1_train_s, H_train_s, H_test = {}, {}, {}, {}\n",
    "H0_train_clf, H1_train_clf, H_train_clf = {}, {}, {}\n",
    "y_train_s = y_train[:n_dir_find]\n",
    "y_train_clf = y_train[start_clf:]\n",
    "for layer in H0_train.keys():\n",
    "    H0_train_s[layer], H1_train_s[layer] = H0_train[layer][:n_dir_find], H1_train[layer][:n_dir_find]\n",
    "    H_train_s[layer] = H0_train[layer][:n_dir_find]-H1_train[layer][:n_dir_find]\n",
    "    H0_train_clf[layer], H1_train_clf[layer] = H0_train[layer][start_clf:], H1_train[layer][start_clf:]\n",
    "    H_train_clf[layer] = H0_train[layer][start_clf:]-H1_train[layer][start_clf:]\n",
    "    H_test[layer] = H0_test[layer]-H1_test[layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211510fc-3154-4b93-9d5b-76ba54063fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing data\n",
    "scalers = {}\n",
    "dscalers = {}\n",
    "for layer in layer_ids:\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.concatenate([H0_train_s[layer], H1_train_s[layer]], axis=0))\n",
    "    scalers[layer] = scaler\n",
    "    H0_train_s[layer] = scaler.transform(H0_train_s[layer])\n",
    "    H1_train_s[layer] = scaler.transform(H1_train_s[layer])\n",
    "    H0_train_clf[layer] = scaler.transform(H0_train_clf[layer])\n",
    "    H1_train_clf[layer] = scaler.transform(H1_train_clf[layer])\n",
    "    H0_test[layer] = scaler.transform(H0_test[layer])\n",
    "    H1_test[layer] = scaler.transform(H1_test[layer])\n",
    "\n",
    "    \n",
    "    dscalers[layer] = StandardScaler()\n",
    "    dscalers[layer].fit(H_train_s[layer])\n",
    "    H_train_s[layer] = dscalers[layer].transform(H_train_s[layer])\n",
    "    H_train_clf[layer] = dscalers[layer].transform(H_train_clf[layer])\n",
    "    H_test[layer] = dscalers[layer].transform(H_test[layer])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22062fd8-440c-4ed9-963e-130b8aa32723",
   "metadata": {},
   "source": [
    "# Finding directions using different methods\n",
    "\n",
    "We find the directions using the hidden representation of our formatted sentences directly or after taking differences between contrastive pairs. To ensure that all directions point towards positive utility we project the training data on the un oriented direction and find the correct coefficient for the orientation using logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae17da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723aee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA on differences of contrastive pairs\n",
    "directions[\"PCA_diffs\"] = {}\n",
    "for layer in tqdm(layer_ids):\n",
    "    pca = PCA(n_components=1, whiten=True).fit(H_train_s[layer])\n",
    "    direction = pca.components_.squeeze()\n",
    "    temp = pca.transform(H_train_s[layer])\n",
    "    lr = LogisticRegression(solver='liblinear').fit(temp, y_train_s)\n",
    "    coeff = np.sign(lr.coef_).squeeze()\n",
    "    directions[\"PCA_diffs\"][layer] = coeff*direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d7ac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA without taking differences\n",
    "directions[\"PCA\"] = {}\n",
    "for layer in tqdm(layer_ids):\n",
    "    pca = PCA(n_components=1, whiten=True).fit(np.concatenate([H0_train_s[layer], H1_train_s[layer]], axis=0))\n",
    "    direction = pca.components_.squeeze()\n",
    "    temp = pca.transform(H_train_s[layer])\n",
    "    lr = LogisticRegression(solver='liblinear').fit(temp, y_train_s)\n",
    "    coeff = np.sign(lr.coef_).squeeze()\n",
    "    directions[\"PCA\"][layer] = coeff*direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658c2367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class means\n",
    "directions[\"ClassMeans\"] = {}\n",
    "for layer in tqdm(layer_ids):\n",
    "    direction = H_train_s[layer][y_train_s==1].mean(axis=0) - H_train_s[layer][y_train_s==0].mean(axis=0)\n",
    "    directions[\"ClassMeans\"][layer] = direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e516ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means\n",
    "directions[\"K-Means\"] = {}\n",
    "for layer in tqdm(layer_ids):\n",
    "    kmeans = KMeans(n_clusters=2, n_init=10).fit(np.concatenate([H0_train_s[layer], H1_train_s[layer]], axis=0))\n",
    "    direction = kmeans.cluster_centers_[0] - kmeans.cluster_centers_[1]\n",
    "    # project onto direction\n",
    "    temp = np.dot(H_train_s[layer], direction.squeeze().T)\n",
    "    lr = LogisticRegression(solver='liblinear').fit(temp.reshape(-1, 1), y_train_s)\n",
    "    coeff = np.sign(lr.coef_).squeeze()\n",
    "    directions[\"K-Means\"][layer] = direction*coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb77cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one prompt method\n",
    "\n",
    "H0_prompt = calc_hidden_states(wrapped_model, [\"Love\"], format_prompt, batch_size=batch_size, token_pos=token_pos)\n",
    "H1_prompt = calc_hidden_states(wrapped_model, [\"Hate\"], format_prompt, batch_size=batch_size, token_pos=token_pos)\n",
    "\n",
    "directions[\"OnePrompt\"] = {}\n",
    "\n",
    "for layer in tqdm(layer_ids):\n",
    "    H0_prompt[layer] = scalers[layer].transform(H0_prompt[layer])\n",
    "    H1_prompt[layer] = scalers[layer].transform(H1_prompt[layer])\n",
    "    direction = H0_prompt[layer]-H1_prompt[layer]\n",
    "    direction = dscalers[layer].transform(direction)\n",
    "    directions[\"OnePrompt\"][layer] = direction.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104eeb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "directions[\"LogReg\"] = {}\n",
    "for layer in tqdm(layer_ids):\n",
    "    lr = LogisticRegression(solver='liblinear', C=1e-2, random_state=0, max_iter=50).fit(H_train_s[layer], y_train_s)\n",
    "    coeff = np.sign(lr.coef_).squeeze()\n",
    "    directions[\"LogReg\"][layer] = coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c22fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random method\n",
    "directions[\"Random\"] = {}\n",
    "num_random_directions = 10\n",
    "for layer in tqdm(layer_ids):\n",
    "    direction = np.random.normal(0.0, 1.0, size=(num_random_directions, hidden_dim))\n",
    "    for i in range(direction.shape[0]):\n",
    "        # project data onto direction\n",
    "        Htr_i = np.dot(H_train_s[layer], direction[i].squeeze().T)\n",
    "        lr = LogisticRegression(solver='liblinear').fit(Htr_i.reshape(-1, 1), y_train_s)\n",
    "        coeff = np.sign(lr.coef_).squeeze()\n",
    "        direction[i] = coeff*direction[i]\n",
    "    directions[\"Random\"][layer] = direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73bf17-3ca3-4b2e-8885-0455b3942497",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = results_dir + f'utility_directions_{model_name}.pkl'\n",
    "with open(fname, 'wb') as f:\n",
    "    pickle.dump(directions, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fb094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = results_dir + f'utility_directions_{model_name}.pkl'\n",
    "\n",
    "with open(fname, 'rb') as f:\n",
    "    directions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3b6499",
   "metadata": {},
   "source": [
    "### Cosine similarity between directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d844bb-eadf-41ee-9a7d-6d1b159c548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "# calculate cosine similarity between directions\n",
    "for layer in tqdm(range(num_layers)):\n",
    "    temp = {key : directions[key][layer] for key in directions.keys()}\n",
    "    temp.pop(\"Random\")\n",
    "    for key in temp.keys():\n",
    "        temp[key] = temp[key].squeeze()\n",
    "    df = pd.DataFrame.from_dict(temp, orient='index')\n",
    "    cosine_sim_matrix = cosine_similarity(df.values)\n",
    "    cosine_sim_df = pd.DataFrame(cosine_sim_matrix, index=df.index, columns=df.index)\n",
    "    dfs.append(cosine_sim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc25d64-1a42-4f6e-a655-6de7e4ad338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(dfs[20], annot=True)\n",
    "plt.savefig(plots_dir + f'utility_cossine_{model_name}_selection.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6150446d-f260-44c0-9e72-c21433a04cb1",
   "metadata": {},
   "source": [
    "# Classification - Test for Correlation\n",
    "How well can the found directions separate the data? We test on differences of the hidden representation of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5af504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(H, y, direction):\n",
    "    Hte_i = np.dot(H, direction.T)\n",
    "    accte = ((Hte_i > 0) == y).sum()/len(y)\n",
    "    return accte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d58a47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accs = {}\n",
    "for method in tqdm(directions.keys()):\n",
    "    test_accs[method] = {}\n",
    "    for layer in layer_ids:\n",
    "        if method == \"Random\":\n",
    "            temp = 0\n",
    "            random_runs = directions[method][layer].shape[0]\n",
    "            for i in range(random_runs):\n",
    "                temp += classification(H_test[layer], y_test, directions[method][layer][i])\n",
    "            test_accs[method][layer] = temp/random_runs\n",
    "        else:\n",
    "            test_accs[method][layer] = classification(H_test[layer], y_test, directions[method][layer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f410c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lines(test_accs, \"test accuracy\", save_path=plots_dir + f'baselines_utility_{model_name}.pdf', method_names=directions.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bf2dcd",
   "metadata": {},
   "source": [
    "# Erasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92de0c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project onto hyperplane perpendicular to direction\n",
    "def project(activation, direction):\n",
    "    dot_product = np.dot(activation, direction)\n",
    "    squared_norm = np.sum(direction ** 2)\n",
    "    projection = np.outer(dot_product / squared_norm, direction)\n",
    "    output = activation - projection\n",
    "    return output     \n",
    "\n",
    "# linear regression\n",
    "def linear_regr(H_trainl, H_testl, y_train, y_test):\n",
    "    scalerem = StandardScaler()\n",
    "    H_trainl = scalerem.fit_transform(H_trainl)\n",
    "    H_testl = scalerem.transform(H_testl)\n",
    "    clf = LogisticRegression(solver='liblinear', C=1e-2, random_state=0, max_iter=50).fit(H_trainl, y_train)\n",
    "    acctr = clf.score(H_trainl, y_train)\n",
    "    accte = clf.score(H_testl, y_test)\n",
    "    return acctr, accte\n",
    "\n",
    "def erase_concept(H_train, H_test, y_train, y_test, direction=None, layers=list(range(model.config.num_hidden_layers))):\n",
    "    train_acc_l, test_acc_l = {}, {}\n",
    "    for layer in tqdm(layers): \n",
    "        if direction is not None:\n",
    "            if len(direction[layer].shape) > 1:\n",
    "                train_acc_temp, test_acc_temp = 0.0, 0.0\n",
    "                for i in range(direction[layer].shape[0]):\n",
    "                    H_train_l = project(H_train[layer], direction[layer][i])\n",
    "                    H_test_l = project(H_test[layer], direction[layer][i])\n",
    "                    acctr, accte = linear_regr(H_train_l, H_test_l, y_train, y_test)\n",
    "                    train_acc_temp += acctr\n",
    "                    test_acc_temp += accte\n",
    "                train_acc_l[layer]= train_acc_temp/direction[layer].shape[0]\n",
    "                test_acc_l[layer] = test_acc_temp/direction[layer].shape[0]\n",
    "            else:\n",
    "                H_train_l = project(H_train[layer], direction[layer])\n",
    "                H_test_l = project(H_test[layer], direction[layer])\n",
    "                acctr, accte = linear_regr(H_train_l, H_test_l, y_train, y_test)\n",
    "                train_acc_l[layer]= acctr\n",
    "                test_acc_l[layer] = accte\n",
    "\n",
    "        else:\n",
    "            acctr, accte = linear_regr(H_train[layer], H_test[layer], y_train, y_test)\n",
    "            train_acc_l[layer]= acctr\n",
    "            test_acc_l[layer] = accte\n",
    "            \n",
    "    return train_acc_l, test_acc_l\n",
    "    \n",
    "def leace(H_train_s, y_train_s, H_train_clf, y_train_clf, H_test, y_test, layers=list(range(model.config.num_hidden_layers))):\n",
    "    train_acc_l, test_acc_l = {}, {}\n",
    "    for layer in tqdm(layers): \n",
    "        H_trainl, H_train_clfl, H_testl = H_train_s[layer], H_train_clf[layer], H_test[layer]\n",
    "        # eraser is trained on first half of training set\n",
    "        eraser = LeaceEraser.fit(torch.from_numpy(H_trainl), torch.from_numpy(y_train_s))\n",
    "        # erase from second half of training set and test set\n",
    "        H_train_clf_tch = eraser(torch.from_numpy(H_train_clfl))\n",
    "        H_test_tch = eraser(torch.from_numpy(H_testl))\n",
    "        H_train_clfl, H_testl = torch.Tensor.numpy(H_train_clf_tch), torch.Tensor.numpy(H_test_tch)\n",
    "        \n",
    "        acctr, accte = linear_regr(H_train_clfl, H_testl, y_train_clf, y_test)\n",
    "        train_acc_l[layer] = acctr\n",
    "        test_acc_l[layer] = accte\n",
    "            \n",
    "    return train_acc_l, test_acc_l\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92699033",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accs, test_accs = {}, {}\n",
    "layer_ids = [0, 5, 10, 15, 20, 25, 30] # which layers to test removal on\n",
    "\n",
    "for key in directions.keys():\n",
    "    print(key)\n",
    "    train_accs[key], test_accs[key] = erase_concept(H_train_clf, H_test, y_train_clf, y_test, directions[key], layers=layer_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3157128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline without erasure\n",
    "train_accs[\"NoErasure\"], test_accs[\"NoErasure\"] = erase_concept(H_train_clf, H_test, y_train_clf, y_test, layers=layer_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99f622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# erase with leace\n",
    "train_accs[\"LEACE\"], test_accs[\"LEACE\"] = leace(H_train_s, y_train_s, H_train_clf, y_train_clf, H_test, y_test, layers=layer_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64620040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "fname = results_dir + f'utility_removal_{model_name}.pkl'\n",
    "with open(fname, 'wb') as f:\n",
    "    pickle.dump([train_accs, test_accs], f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38912cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plot_lines(train_accs, \"Train accuracy\", save_path=plots_dir + f'utility_removal_train_accs_{model_name}.png', method_names=train_accs.keys())\n",
    "plot_lines(test_accs, \"Test accuracy\", save_path=plots_dir + f'utility_removal_test_accs_{model_name}.png', method_names=test_accs.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
