{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8812879",
   "metadata": {},
   "source": [
    "# Steering with previously calculated directions\n",
    "We apply activation addition to steer the generated text into positve and negative concept directions respectively.\n",
    "We evaluate the generated text on coherence and content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9de3f9",
   "metadata": {},
   "source": [
    "### User data\n",
    "You need to specify the current working directory and the huggingface [access token](https://huggingface.co/docs/hub/security-tokens) to use this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4087c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify to your current working directory (the directory where this notebook is )\n",
    "cwd = \"exploring_directions\"\n",
    "\n",
    "# enter your authentication token from huggingface and press enter to access the models\n",
    "auth_token = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d78bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, AutoConfig\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import gc\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from scipy.special import softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff64be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import my modules\n",
    "import sys\n",
    "# join the path to the modules to the current working directory\n",
    "\n",
    "sys.path.append(os.path.join(cwd, \"modules\"))\n",
    "\n",
    "from wrapping import WrappedModel\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0ead65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directories\n",
    "results_dir = make_dir(os.path.join(cwd, 'results/'))\n",
    "generations_dir = make_dir(os.path.join(results_dir, 'generations/'))\n",
    "plots_dir = make_dir(os.path.join(cwd, 'plots/'))\n",
    "data_dir = make_dir(os.path.join(cwd, 'data/'))\n",
    "\n",
    "# parameters for steering/generation\n",
    "data_file = os.path.join(data_dir, 'test_sentences.txt')\n",
    "num_test_sentences = 500\n",
    "random_seed = 42\n",
    "calc_generations = True\n",
    "block_name = \"decoder_block\"\n",
    "max_new_tokens = 40 # how many tokens to generate while steering\n",
    "layer_ids = [0, 5, 10, 15, 20, 25, 30] # which layers to steer\n",
    "batch_size = 128\n",
    "# directions have different norms for different methods. We need to choose coefficients appropriately\n",
    "# we can take the norms of one method that has relation to actual differences in hidden layers as coefficients for all methods\n",
    "norm_method = \"ClassMeans\"\n",
    "# we use norms of ClassMeans directions as coefficients, but utility is based on differences, so we need to divide by 2 \n",
    "multiplier = 0.5\n",
    "\n",
    "# set to True if you want to evaluate the generated data\n",
    "evaluate_logprobs = True\n",
    "evaluate_sentiment = True\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7431c42a",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "We need some setup to generate sentences. Like the beginning of a scenario, that we then generate the end to while adding a steering vector. We can just get some sentences from the utility test set which are easily divisible into two parts, throw away the second part and use the first part as the generation seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6679f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data\n",
    "X_test = load_util_data(data_dir=os.path.join(cwd, 'data/ethics/utilitarianism/'), split='test')\n",
    "X_test, y_test = mix_util_pairs(X_test)\n",
    "\n",
    "test_data_idxs, split_sentences = find_two_sentences(X_test[:, 0], split_str1=\".\", split_str2=\",\", larger_than1=2, larger_than2=1)\n",
    "\n",
    "with open(data_file, \"w\") as f:\n",
    "    for s in split_sentences:\n",
    "        f.write(s + \" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adb0c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open(data_file, 'r') as f:\n",
    "    test_sentences = [line.strip() for line in f]\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "idxs = np.random.choice(len(test_sentences), num_test_sentences, replace=False)\n",
    "test_sentences = [test_sentences[idx] for idx in idxs]\n",
    "for i in range(10):\n",
    "    print(test_sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c958a10",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3965aad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_name = \"Llama-2-7b-chat-hf\"\n",
    "model_path = f\"meta-llama/{model_name}\"\n",
    "precision = torch.bfloat16\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, token=auth_token, device_map=\"auto\").to(device=device, dtype=precision)\n",
    "model.eval()\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, token=auth_token, device_map=\"auto\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left' \n",
    "\n",
    "num_hidden_layers = model.config.num_hidden_layers\n",
    "hidden_size = model.config.hidden_size\n",
    "\n",
    "# create wrapped model\n",
    "wrapped_model = WrappedModel(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3441078",
   "metadata": {},
   "source": [
    "# Steering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243b57f7",
   "metadata": {},
   "source": [
    "### Load directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faaca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pkl directions\n",
    "with open(os.path.join(results_dir, f'utility_directions_{model_name}.pkl'), \"rb\") as f:\n",
    "    all_directions = pickle.load(f)\n",
    "\n",
    "# remove random directions\n",
    "if \"Random\" in all_directions:\n",
    "    all_directions.pop(\"Random\", None)\n",
    "\n",
    "method_names = list(all_directions.keys())\n",
    "print(method_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b9d632",
   "metadata": {},
   "source": [
    "### Define multipliers for scaling the steering vectors\n",
    "We need to define the scaling coefficient for each layer separately. We can take the norms of one method that has relation to actual differences in hidden layers as coefficients for all methods for example the class means method. As the class mean norm would be the difference between high utility and low utility examples but we are starting from neutral, we multiply by 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d467d436",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_directions = all_directions[norm_method]\n",
    "# convert dict to tensor\n",
    "norm_directions = {k: torch.tensor(v).to(device=device, dtype=precision) for k, v in norm_directions.items()}\n",
    "\n",
    "coeffs = {k:multiplier*v.norm().squeeze() for k, v in norm_directions.items()}\n",
    "\n",
    "# Use Seaborn to set the style and context\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\")\n",
    "\n",
    "# make data frame for coefficients\n",
    "df_coeffs = pd.DataFrame.from_dict({k: v.item() for k, v in coeffs.items()}, orient='index', columns=['coeff'])\n",
    "df_coeffs.plot(kind='line', marker='o', figsize=(8, 5))\n",
    "\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel(f\"Norm '{norm_method}'\")\n",
    "plt.grid(True)\n",
    "# switch off legend\n",
    "plt.legend().set_visible(False)\n",
    "\n",
    "plt.savefig(os.path.join(plots_dir, f\"norm_{norm_method}_{model_name}.png\"), dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c66279",
   "metadata": {},
   "source": [
    "### Completion with activation addition\n",
    "\n",
    "We add the scaled steering vectors for each method and layer seperately in the positive and negative direction respectively and generate new tokens for each starting sentence in `test_sentences`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd40bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_generations:\n",
    "    \n",
    "    # completions without activation steering\n",
    "    generations = []\n",
    "    wrapped_model.unwrap()\n",
    "    for sentence_batch in batchify(test_sentences, batch_size):\n",
    "        generated = wrapped_model.generate(sentence_batch, max_new_tokens=max_new_tokens, use_cache=True)\n",
    "        generations.extend(generated)\n",
    "\n",
    "    # save generations as text files\n",
    "    with open(os.path.join(generations_dir, f\"generations_neutral.txt\"), \"w\") as f:\n",
    "        for item in generations:\n",
    "            # remove newline characters\n",
    "            item = item.replace(\"\\n\", \" \")\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "\n",
    "    for method_name in method_names:\n",
    "        \n",
    "        print(f\"method: {method_name}\")\n",
    "\n",
    "        for layer_id in tqdm(layer_ids):\n",
    "\n",
    "            generations = {\"positive\": [], \"negative\": []}\n",
    "\n",
    "            wrapped_model.unwrap()\n",
    "            wrapped_model.wrap_block(layer_id, block_name=block_name)\n",
    "\n",
    "            direction = torch.tensor(all_directions[method_name][layer_id]).to(device=device, dtype=precision)\n",
    "            direction = direction / direction.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            wrapped_model.reset()\n",
    "            wrapped_model.set_to_add(layer_id, coeffs[layer_id]*direction, block_name=block_name)\n",
    "\n",
    "            for sentence_batch in batchify(test_sentences, batch_size):\n",
    "                generated = wrapped_model.generate(sentence_batch, max_new_tokens=max_new_tokens, use_cache=True)\n",
    "                generations[\"positive\"].extend(generated)\n",
    "\n",
    "\n",
    "            wrapped_model.reset()\n",
    "            wrapped_model.set_to_add(layer_id, -coeffs[layer_id]*direction, block_name=block_name)\n",
    "\n",
    "            for sentence_batch in batchify(test_sentences, batch_size):\n",
    "                generated = wrapped_model.generate(sentence_batch, max_new_tokens=max_new_tokens, use_cache=True)\n",
    "                generations[\"negative\"].extend(generated)\n",
    "\n",
    "            # save generations as text files\n",
    "            with open(os.path.join(generations_dir, f\"generations_positive_{method_name}_{layer_id}.txt\"), \"w\") as f:\n",
    "                for item in generations[\"positive\"]:\n",
    "                    # remove newline characters\n",
    "                    item = item.replace(\"\\n\", \" \")\n",
    "                    f.write(\"%s\\n\" % item)\n",
    "\n",
    "            with open(os.path.join(generations_dir, f\"generations_negative_{method_name}_{layer_id}.txt\"), \"w\") as f:\n",
    "                for item in generations[\"negative\"]:\n",
    "                    item = item.replace(\"\\n\", \" \")\n",
    "                    f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e93245c",
   "metadata": {},
   "source": [
    "# Evaluate the coherence/likelihood of the generated text\n",
    "\n",
    "Using the original (non-steered) model, we sum over the log probability of each generated sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad15464",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_model.unwrap()\n",
    "gc.collect()\n",
    "batch_size = 32\n",
    "if evaluate_logprobs:\n",
    "    probs = {}\n",
    "\n",
    "    print(f\"method: Baseline\")\n",
    "    probs['Baseline'] = {}\n",
    "    \n",
    "    all_generations = load_generations(os.path.join(generations_dir, f\"generations_neutral.txt\"))\n",
    "    logprobs = []\n",
    "    for sentence_batch in batchify(all_generations, batch_size):\n",
    "        gc.collect()\n",
    "        logits, attention_mask, input_ids = get_logits(tokenizer, wrapped_model, sentence_batch, device)\n",
    "        p = get_logprobs(logits, input_ids, attention_mask)\n",
    "        logprobs.extend(list(p.sum(dim=-1).detach().cpu().float().numpy()))\n",
    "\n",
    "    for layer_id in layer_ids:\n",
    "        probs['Baseline'][layer_id] = np.mean(logprobs)\n",
    "\n",
    "\n",
    "    for method_name in method_names:\n",
    "        gc.collect()\n",
    "        probs[method_name] = {}\n",
    "        print(f\"method: {method_name}\")\n",
    "        for layer_id in tqdm(layer_ids):\n",
    "            gc.collect()\n",
    "\n",
    "            # check if file exists\n",
    "            if not os.path.exists(os.path.join(generations_dir, f\"generations_positive_{method_name}_{layer_id}.txt\")):\n",
    "                print(f\"File not found: {os.path.join(generations_dir, f'generations_positive_{method_name}_{layer_id}.txt')}\")\n",
    "                continue\n",
    "\n",
    "            # load generations\n",
    "            all_generations = load_generations(os.path.join(generations_dir, f\"generations_positive_{method_name}_{layer_id}.txt\"))\n",
    "            all_generations.extend(load_generations(os.path.join(generations_dir, f\"generations_negative_{method_name}_{layer_id}.txt\")))\n",
    "\n",
    "            # eval\n",
    "            logprobs = []\n",
    "            for sentence_batch in batchify(all_generations, batch_size):\n",
    "                logits, attention_mask, input_ids = get_logits(tokenizer, wrapped_model, sentence_batch, device)\n",
    "                p = get_logprobs(logits, input_ids, attention_mask)\n",
    "                logprobs.extend(list(p.sum(dim=-1).detach().cpu().float().numpy()))\n",
    "\n",
    "            probs[method_name][layer_id] = np.mean(logprobs)\n",
    "    # save probs\n",
    "    with open(os.path.join(results_dir, f'logprobs_{model_name}.pkl'), \"wb\") as f:\n",
    "        pickle.dump(probs, f)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4aa38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load probs\n",
    "with open(os.path.join(results_dir, f'logprobs_{model_name}.pkl'), \"rb\") as f:\n",
    "    probs = pickle.load(f)\n",
    "plot_lines(probs, \"Log probabilities\", os.path.join(plots_dir, f\"probs_{model_name}.png\"), method_names=probs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b8533e",
   "metadata": {},
   "source": [
    "# Sentiment analysis with sentiment model\n",
    "\n",
    "We do sentiment analysis with a classifier based on the RoBERTa model. There are three output classes: negative, neutral and positive. We focus on the probability for the positive output check that positively steered generated text has higher positive output than negatively steered generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a882867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left' \n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device=device, dtype=precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7b4973",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluate_sentiment:\n",
    "\n",
    "    sentiment_accs = {}\n",
    "\n",
    "    for method_name in method_names:\n",
    "        gc.collect()\n",
    "        sentiment_accs[method_name] = {}\n",
    "        print(f\"method: {method_name}\")\n",
    "        for layer_id in tqdm(layer_ids):\n",
    "            gc.collect()\n",
    "\n",
    "            # check if file exists\n",
    "            if not os.path.exists(os.path.join(generations_dir, f\"generations_positive_{method_name}_{layer_id}.txt\")):\n",
    "                print(f\"File not found: {os.path.join(generations_dir, f'generations_positive_{method_name}_{layer_id}.txt')}\")\n",
    "                continue\n",
    "\n",
    "            # load generations\n",
    "            generations = {\"positive\": [], \"negative\": []}\n",
    "            generations[\"positive\"] = load_generations(os.path.join(generations_dir, f\"generations_positive_{method_name}_{layer_id}.txt\"))\n",
    "            generations[\"negative\"] = load_generations(os.path.join(generations_dir, f\"generations_negative_{method_name}_{layer_id}.txt\"))\n",
    "\n",
    "            outputs = {\"positive\": [], \"negative\": []}\n",
    "            # eval\n",
    "            for sentence_batch in batchify(generations[\"positive\"], batch_size):\n",
    "                logits, _, _ = get_logits(tokenizer, sentiment_model, sentence_batch, device)\n",
    "                output = softmax(logits.detach().float().cpu().numpy(), axis=-1)\n",
    "                outputs[\"positive\"].append(output)\n",
    "\n",
    "            outputs[\"positive\"] = np.concatenate(outputs[\"positive\"], axis=0)\n",
    "\n",
    "            for sentence_batch in batchify(generations[\"negative\"], batch_size):\n",
    "                logits, _, _ = get_logits(tokenizer, sentiment_model, sentence_batch, device)\n",
    "                output = softmax(logits.detach().float().cpu().numpy(), axis=-1)\n",
    "                outputs[\"negative\"].append(output)\n",
    "\n",
    "            outputs[\"negative\"] = np.concatenate(outputs[\"negative\"], axis=0)\n",
    "\n",
    "            # output has three values per sample: negative, neutral, positive\n",
    "            # calculate accuracy\n",
    "            sentiment_accs[method_name][layer_id] = np.mean(outputs[\"positive\"][:,-1]>outputs[\"negative\"][:,-1])\n",
    "\n",
    "    # save accs\n",
    "    with open(os.path.join(results_dir, f'sentiment_accs_{model_name}.pkl'), \"wb\") as f:\n",
    "        pickle.dump(sentiment_accs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3ae0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sentiment_accs\n",
    "with open(os.path.join(results_dir, f'sentiment_accs_{model_name}.pkl'), \"rb\") as f:\n",
    "    sentiment_accs = pickle.load(f)\n",
    "plot_lines(sentiment_accs, \"Sentiment accuracy\", os.path.join(plots_dir, f\"sentiment_accs_{model_name}.png\"), method_names=sentiment_accs.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
